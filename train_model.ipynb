{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac443c-9dec-439f-a968-2a93d985d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 21:57:59,091 - INFO - Using Computation Device: cuda\n",
      "2026-01-02 21:57:59,095 - INFO - GPU: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "2026-01-02 21:57:59,095 - INFO - CUDA Version: 12.4\n",
      "2026-01-02 21:57:59,095 - INFO - Total GPU Memory: 17.17 GB\n"
     ]
    }
   ],
   "source": [
    "# ===== IMPORTS AND ENVIRONMENT SETUP =====\n",
    "# This section imports all necessary libraries and configures the environment\n",
    "\n",
    "# System and file handling libraries\n",
    "import os          # Operating system operations (file paths, directories)\n",
    "import re          # Regular expressions for text pattern matching\n",
    "import time        # Time tracking for performance monitoring\n",
    "import json        # JSON file reading/writing\n",
    "import logging     # Logging framework for tracking progress\n",
    "import joblib      # Saving/loading Python objects (like models)\n",
    "import shutil      # High-level file operations\n",
    "import collections # Specialized container datatypes (Counter, etc.)\n",
    "import gc          # Garbage collector for memory management\n",
    "\n",
    "# Numerical and data processing\n",
    "import numpy as np       # Numerical arrays and operations\n",
    "import pandas as pd      # Dataframes for tabular data\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "import seaborn as sns    # Statistical data visualization\n",
    "\n",
    "# PyTorch - Deep Learning Framework\n",
    "import torch             # Main PyTorch module\n",
    "import torch.nn as nn    # Neural network layers and modules\n",
    "import torch.optim as optim  # Optimization algorithms (Adam, SGD, etc.)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Data loading utilities\n",
    "\n",
    "# Machine Learning preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Text to numerical features\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "\n",
    "# Parallel processing and progress tracking\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # Multi-threading\n",
    "from pathlib import Path    # Modern file path handling\n",
    "from tqdm.auto import tqdm  # Progress bars\n",
    "\n",
    "# ===== LOGGING SETUP =====\n",
    "# Configure logging to track what the notebook is doing\n",
    "# Level INFO = show informational messages, warnings, and errors\n",
    "# Format = timestamp - level - message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== CHECKPOINT CONFIGURATION =====\n",
    "# Checkpoints allow us to save intermediate results and avoid recomputation\n",
    "# If the notebook crashes, we can resume from the last checkpoint\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "    logger.info(f\"‚úÖ Created checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# ===== CONSTANTS =====\n",
    "# Define all file paths and global settings in one place\n",
    "BASE_DIR = \"data/kafka\"                      # Where raw log files are stored\n",
    "MODEL_PATH = \"data/anomaly_autoencoder.pth\"  # Where to save the final trained model\n",
    "VECTORIZER_PATH = \"data/tfidf_vectorizer.pkl\"  # TF-IDF model path (if used)\n",
    "SCALER_PATH = \"data/scaler.pkl\"              # Data scaler path (if used)\n",
    "SEED = 42  # Random seed for reproducibility (same results every run)\n",
    "\n",
    "# ===== REPRODUCIBILITY SETUP =====\n",
    "# Set random seeds so that results are consistent across runs\n",
    "# This ensures that random initialization, shuffling, etc. produce the same results\n",
    "torch.manual_seed(SEED)          # PyTorch CPU random seed\n",
    "np.random.seed(SEED)             # NumPy random seed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)           # PyTorch GPU random seed\n",
    "    torch.backends.cudnn.deterministic = True  # Force deterministic GPU operations\n",
    "    torch.backends.cudnn.benchmark = False     # Disable auto-tuning for consistency\n",
    "\n",
    "# ===== DEVICE CONFIGURATION =====\n",
    "# Check if GPU is available and configure accordingly\n",
    "# GPU (CUDA) = much faster training, CPU = slower but always available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using Computation Device: {DEVICE}\")\n",
    "\n",
    "# If GPU is available, print GPU information\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")  # GPU model name\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")     # CUDA version\n",
    "    logger.info(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Enable TF32 (TensorFloat-32) for better performance on modern GPUs (RTX 30/40 series)\n",
    "    # TF32 provides faster computation with minimal accuracy loss\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Unsupervised Log Anomaly Detection Pipeline\n",
    "\n",
    "This notebook implements an end-to-end pipeline to detect anomalies in logs without labeled data (Unsupervised Learning).\n",
    "\n",
    "## Architecture\n",
    "1. **Ingestion**: Load raw logs from text files.\n",
    "2. **Cleaning (Optimized)**: Uses **RAPIDS (GPU)** if available, or **Dask (Multi-core CPU)** to clean logs via Regex.\n",
    "3. **Vectorization**: TF-IDF to convert log text into numerical vectors.\n",
    "4. **Modeling**: A PyTorch **Autoencoder** learns the \"normal\" structure of logs.\n",
    "5. **Anomaly Scoring**: The reconstruction error (MSE) serves as the anomaly score. High error = Rare/Abnormal log.\n",
    "\n",
    "## Checkpoint System\n",
    "\n",
    "This notebook includes automatic checkpointing to save intermediate results:\n",
    "- **data.pkl**: Raw loaded data\n",
    "- **cleaned_data.pkl**: Cleaned logs after regex processing\n",
    "- **vocab.json**: Vocabulary dictionary\n",
    "- **encoded_logs.npy**: Encoded input IDs matrix\n",
    "\n",
    "Checkpoints are stored in `./checkpoints/` directory. Delete individual checkpoint files to force recomputation of that step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 21:57:59,117 - INFO - Using Computation Device: cuda\n",
      "2026-01-02 21:57:59,117 - INFO - GPU: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "2026-01-02 21:57:59,119 - INFO - CUDA Version: 12.4\n",
      "2026-01-02 21:57:59,119 - INFO - Total GPU Memory: 17.17 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "BASE_DIR = \"data/kafka\"\n",
    "MODEL_PATH = \"data/anomaly_autoencoder.pth\"\n",
    "VECTORIZER_PATH = \"data/tfidf_vectorizer.pkl\"\n",
    "SCALER_PATH = \"data/scaler.pkl\"\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check Device and GPU Info\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using Computation Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    logger.info(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # Enable TF32 for better performance on Ampere+ GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_df(path):\n",
    "    \"\"\"\n",
    "    Reads a single text file directly into a DataFrame.\n",
    "    \n",
    "    Purpose: Convert a raw log file (.txt) into a pandas DataFrame with one column 'raw_log'.\n",
    "    Each line in the file becomes a row in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the log file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with column 'raw_log' containing the log lines\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(\n",
    "            path, \n",
    "            header=None,              # File has no header row\n",
    "            names=['raw_log'],        # Name the column 'raw_log'\n",
    "            sep='\\t',                 # Tab-separated (adjust if different)\n",
    "            engine='python',          # Use Python engine for flexible parsing\n",
    "            dtype='str',              # Treat all data as strings\n",
    "            encoding='utf-8',         # UTF-8 encoding for special characters\n",
    "            encoding_errors='ignore', # Ignore problematic characters instead of crashing\n",
    "            on_bad_lines='skip'       # Skip malformed lines instead of crashing\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {path}: {e}\")\n",
    "        # Return empty DataFrame if file cannot be read\n",
    "        return pd.DataFrame(columns=['raw_log'])\n",
    "\n",
    "def load_data(base_dir):\n",
    "    \"\"\"\n",
    "    Loads all log files from base_dir in parallel using multi-threading.\n",
    "    \n",
    "    Purpose: Efficiently load thousands of .txt files simultaneously using multiple CPU cores.\n",
    "    This is much faster than loading files one by one.\n",
    "    \n",
    "    Workflow:\n",
    "        1. Find all .txt files in the directory tree\n",
    "        2. Read each file in parallel using ThreadPoolExecutor\n",
    "        3. Combine all DataFrames into one large DataFrame\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Root directory containing log files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all logs combined\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        # If data directory doesn't exist, create dummy data for testing\n",
    "        logger.warning(\"Data dir not found. Generating dummy logs.\")\n",
    "        return pd.DataFrame({'raw_log': [\n",
    "            \"2023-10-27T10:00:00.123Z [1234] PID 9999 INFO Connection established\",\n",
    "            \"2023-10-27T10:00:01.000Z [1234] PID 9999 ERROR Connection refused 10.0.0.1\"\n",
    "        ] * 5000})  # Repeat dummy logs 5000 times\n",
    "\n",
    "    # Step 1: Find all .txt files recursively\n",
    "    base_path = Path(base_dir)\n",
    "    files = list(base_path.rglob('*.txt'))  # rglob = recursive glob (searches subdirectories)\n",
    "    \n",
    "    logger.info(f\"Found {len(files)} .txt files.\")\n",
    "\n",
    "    # Step 2: Read files in parallel using thread pool\n",
    "    dfs = []  # List to collect DataFrames from each file\n",
    "    \n",
    "    # ThreadPoolExecutor creates a pool of worker threads for parallel execution\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit all file read tasks to the executor\n",
    "        future_to_path = {executor.submit(read_file_to_df, f): f for f in files}\n",
    "        \n",
    "        # Process completed tasks as they finish (not in submission order)\n",
    "        for future in tqdm(as_completed(future_to_path), total=len(files), desc=\"Reading Files\"):\n",
    "            try:\n",
    "                df = future.result()  # Get the DataFrame from completed task\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process file: {e}\")\n",
    "\n",
    "    # Step 3: Efficiently combine all DataFrames into one\n",
    "    if dfs:\n",
    "        logger.info(\"Concatenating DataFrames...\")\n",
    "        \n",
    "        with tqdm(total=1, desc=\"Merging Data\") as pbar:\n",
    "            # pd.concat combines DataFrames vertically (stacking rows)\n",
    "            # ignore_index=True renumbers rows from 0 to N\n",
    "            df = pd.concat(dfs, ignore_index=True)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    else:\n",
    "        # No files were successfully read\n",
    "        df = pd.DataFrame(columns=['raw_log'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6b2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 21:57:59,203 - INFO - üîÑ Loading data from source files...\n",
      "2026-01-02 21:57:59,261 - INFO - Found 12302 .txt files.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b8c53264e2474ebe73dd8a1c24508a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/12302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:34:45,133 - INFO - Concatenating DataFrames...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486f281993184d43a9e6f418f3c9438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging Data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load data from scratch (skip pickle to avoid MemoryError with large datasets)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# NOTE: We don't save to pickle because the file becomes too large (>16GB)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Instead, we work directly with encoded_logs.npy which is memory-mapped\u001b[39;00m\n\u001b[32m      4\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Loading data from source files...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m logs from source files.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(base_dir)\u001b[39m\n\u001b[32m     81\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConcatenating DataFrames...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[32m1\u001b[39m, desc=\u001b[33m\"\u001b[39m\u001b[33mMerging Data\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m     84\u001b[39m         \u001b[38;5;66;03m# pd.concat combines DataFrames vertically (stacking rows)\u001b[39;00m\n\u001b[32m     85\u001b[39m         \u001b[38;5;66;03m# ignore_index=True renumbers rows from 0 to N\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m         pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# No files were successfully read\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\concat.py:185\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         values = concat_compat(vals, axis=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     values = \u001b[43mensure_wrapped_if_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     fastpath = blk.values.dtype == values.dtype\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\construction.py:481\u001b[39m, in \u001b[36mensure_wrapped_if_datetimelike\u001b[39m\u001b[34m(arr)\u001b[39m\n\u001b[32m    476\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m obj.to_numpy()  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensure_wrapped_if_datetimelike\u001b[39m(arr):\n\u001b[32m    482\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[33;03m    Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray.\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, np.ndarray):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load data from scratch (skip pickle to avoid MemoryError with large datasets)\n",
    "# NOTE: We don't save to pickle because the file becomes too large (>16GB)\n",
    "# Instead, we work directly with encoded_logs.npy which is memory-mapped\n",
    "logger.info(f\"üîÑ Loading data from source files...\")\n",
    "df = load_data(BASE_DIR)\n",
    "logger.info(f\"‚úÖ Loaded {len(df)} logs from source files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_logic",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning (GPU/CPU Hybrid)\n",
    "We implement the cleaning logic with a fallback mechanism defined in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4182456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:23:42,360 - INFO - Data shape: (246979255, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "raw_log",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8a8dac07-6512-4926-a0d1-5f2746d7b906",
       "rows": [
        [
         "0",
         "builder: mozilla-esr52_win7_vm-debug_test-mochitest-devtools-chrome-8"
        ],
        [
         "1",
         "slave: t-w732-spot-130"
        ],
        [
         "2",
         "starttime: 1527808669.5"
        ],
        [
         "3",
         "results: success (0)"
        ],
        [
         "4",
         "buildid: 20180531145517"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>builder: mozilla-esr52_win7_vm-debug_test-moch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slave: t-w732-spot-130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>starttime: 1527808669.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>results: success (0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buildid: 20180531145517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_log\n",
       "0  builder: mozilla-esr52_win7_vm-debug_test-moch...\n",
       "1                             slave: t-w732-spot-130\n",
       "2                            starttime: 1527808669.5\n",
       "3                               results: success (0)\n",
       "4                            buildid: 20180531145517"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(f\"Data shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleaning_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA CLEANING PATTERNS =====\n",
    "# These patterns replace variable parts of logs with standardized tokens\n",
    "# This helps the model focus on the structure rather than specific values\n",
    "\n",
    "# Why clean logs?\n",
    "# - Raw logs contain timestamps, IPs, IDs that change constantly\n",
    "# - \"2023-10-27 ERROR\" and \"2023-10-28 ERROR\" should be treated as the same pattern\n",
    "# - By replacing variables with tokens, we reduce vocabulary size and improve learning\n",
    "\n",
    "PATTERNS_PY = {\n",
    "    # Replace ISO8601 timestamps like \"2023-10-27T10:00:00.123Z\" with [TIMESTAMP_ISO]\n",
    "    r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}Z\": \"[TIMESTAMP_ISO]\",\n",
    "    \n",
    "    # Replace standard timestamps like \"2023-10-27 10:00:00\" with [TIMESTAMP]\n",
    "    r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\": \"[TIMESTAMP]\",\n",
    "    \n",
    "    # Replace \"PID 1234\" with \"PID [PID_NUM]\"\n",
    "    r\"PID\\s+\\d+\": \"PID [PID_NUM]\",\n",
    "    \n",
    "    # Replace bracketed numbers like [1234] with [ID]\n",
    "    r\"\\[\\d+\\]\": \"[ID]\",\n",
    "    \n",
    "    # Replace IP addresses like \"192.168.1.1\" with [IP_ADDR]\n",
    "    r\"(?:\\d{1,3}\\.){3}\\d{1,3}\": \"[IP_ADDR]\",\n",
    "    \n",
    "    # Replace multiple spaces with single space for consistency\n",
    "    r\"\\s+\": \" \"\n",
    "}\n",
    "\n",
    "def smart_clean_to_disk(df, batch_size=50_000, output_file=None):\n",
    "    \"\"\"\n",
    "    Cleans log data in batches and saves DIRECTLY to disk to avoid memory overflow.\n",
    "    \n",
    "    Purpose: Process millions of logs without running out of RAM.\n",
    "    Instead of loading all cleaned logs in memory, we write them incrementally to a CSV file.\n",
    "    \n",
    "    Workflow:\n",
    "        1. Process data in small batches (50,000 rows at a time)\n",
    "        2. Apply regex patterns to clean each batch\n",
    "        3. Immediately write cleaned batch to disk\n",
    "        4. Repeat until all data is processed\n",
    "        5. Read back the complete cleaned CSV\n",
    "    \n",
    "    GPU Acceleration:\n",
    "        - If GPU is available and supports text operations, use it\n",
    "        - Otherwise, fall back to CPU (regex operations)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'raw_log' column\n",
    "        batch_size: Number of rows to process at once (smaller = less memory)\n",
    "        output_file: Where to save cleaned data (default: checkpoints/temp_cleaned.csv)\n",
    "    \n",
    "    Returns:\n",
    "        Series containing cleaned logs\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = f\"{CHECKPOINT_DIR}/temp_cleaned.csv\"\n",
    "    \n",
    "    # Check if GPU is available for text processing\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_gpu = False\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        try:\n",
    "            # Test if PyTorch supports GPU string operations\n",
    "            # (Not all PyTorch versions/platforms support this)\n",
    "            _ = torch.string\n",
    "            use_gpu = True\n",
    "            logger.info(f\"üî• GPU detected and will be used for text cleaning\")\n",
    "        except AttributeError:\n",
    "            logger.warning(\"‚ö†Ô∏è Windows detected: GPU not supported for text. Using CPU.\")\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Delete existing cleaned file if it exists (start fresh)\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Create CSV file with header\n",
    "    pd.DataFrame(columns=['cleaned_log']).to_csv(output_file, index=False)\n",
    "    \n",
    "    # Process data in batches with progress bar\n",
    "    with tqdm(total=total_rows, unit=\"rows\", desc=\"Cleaning\") as pbar:\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch_end = min(i + batch_size, total_rows)\n",
    "            \n",
    "            # Extract current batch\n",
    "            chunk = df['raw_log'].iloc[i:batch_end].fillna('').astype(str)\n",
    "            \n",
    "            # Try GPU processing first\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    # Convert to GPU tensor\n",
    "                    logs_tensor = torch.as_tensor(chunk.tolist(), dtype=torch.string, device=device)\n",
    "                    cleaned_tensor = logs_tensor\n",
    "                    \n",
    "                    # Apply all regex patterns on GPU\n",
    "                    for pat, repl in PATTERNS_PY.items():\n",
    "                        cleaned_tensor = torch.strings.regex_replace(cleaned_tensor, pat, repl)\n",
    "                    \n",
    "                    # Convert back to pandas Series\n",
    "                    cleaned_series = pd.Series(cleaned_tensor.tolist(), name='cleaned_log')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # GPU processing failed, fall back to CPU\n",
    "                    logger.warning(f\"GPU error ({e}), falling back to CPU...\")\n",
    "                    use_gpu = False\n",
    "            \n",
    "            # CPU processing (or GPU fallback)\n",
    "            if not use_gpu:\n",
    "                # Apply regex patterns sequentially using pandas string methods\n",
    "                for pat, repl in PATTERNS_PY.items():\n",
    "                    chunk = chunk.str.replace(pat, repl, regex=True)\n",
    "                cleaned_series = chunk.rename('cleaned_log')\n",
    "            \n",
    "            # Append cleaned batch to CSV file (append mode = 'a')\n",
    "            cleaned_series.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(batch_end - i)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Cleaning complete. File saved: {output_file}\")\n",
    "    \n",
    "    # Read the complete cleaned file back into memory\n",
    "    logger.info(\"üîÑ Reading cleaned file back into memory...\")\n",
    "    df_cleaned = pd.read_csv(output_file)\n",
    "\n",
    "    return df_cleaned['cleaned_log']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea0112",
   "metadata": {},
   "source": [
    "## Data Cleaning Function Details\n",
    "\n",
    "The `smart_clean_to_disk` function:\n",
    "1. **Objective**: Clean a column of logs (raw_log) by replacing variable elements (dates, IP, IDs) with standardized tokens ([TIMESTAMP], etc.)\n",
    "2. **Batch Processing**: Processes data in chunks to avoid memory saturation\n",
    "3. **GPU Fallback**: Automatically switches to CPU processing if GPU text processing fails\n",
    "4. **Progress Tracking**: Real-time progress bar with tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe999f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:23:42,500 - INFO - Data shape before cleaning: (246979255, 1)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Data shape before cleaning: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99561b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:23:42,521 - INFO - üìÇ Loading cached cleaned data from CSV...\n",
      "2026-01-02 15:32:24,184 - INFO - ‚úÖ Loaded cleaned data from CSV checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# Load or compute cleaned logs\n",
    "# NOTE: Skip pickle checkpoint (too large). Load from CSV temp file if available\n",
    "checkpoint_file_csv = f\"{CHECKPOINT_DIR}/temp_cleaned.csv\"\n",
    "if os.path.exists(checkpoint_file_csv):\n",
    "    try:\n",
    "        logger.info(f\"üìÇ Loading cached cleaned data from CSV...\")\n",
    "        df['cleaned_log'] = pd.read_csv(checkpoint_file_csv)['cleaned_log']\n",
    "        logger.info(f\"‚úÖ Loaded cleaned data from CSV checkpoint.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Failed to load CSV ({e}). Recomputing...\")\n",
    "        logger.info(f\"üîÑ Computing cleaned logs from scratch...\")\n",
    "        df['cleaned_log'] = smart_clean_to_disk(df)\n",
    "else:\n",
    "    logger.info(f\"üîÑ Computing cleaned logs from scratch...\")\n",
    "    df['cleaned_log'] = smart_clean_to_disk(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff875a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:24,332 - INFO - Data shape after cleaning: (246979255, 2)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Data shape after cleaning: {df.shape}\")\n",
    "# df[['raw_log', 'cleaned_log']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453649b1",
   "metadata": {},
   "source": [
    "## 3. Build Vocabulary from Cleaned Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VOCABULARY CONFIGURATION =====\n",
    "# The vocabulary is a dictionary mapping words to unique integer IDs\n",
    "# This allows us to convert text into numbers that the neural network can process\n",
    "\n",
    "VOCAB_SIZE_LIMIT = 50000    # Maximum words in dictionary (top 50k most common)\n",
    "MIN_OCCURRENCES = 5         # Ignore rare words (appearing less than 5 times)\n",
    "SAMPLE_SIZE = None          # None = Use ALL rows, or set a number for faster testing\n",
    "TEXT_COLUMN = 'cleaned_log' # Which column contains the text to analyze\n",
    "OUTPUT_FILE = f\"{CHECKPOINT_DIR}/vocab_full.json\"  # Where to save vocabulary\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"  # Padding token (ID: 0) - used to fill short sequences to max length\n",
    "UNK_TOKEN = \"<UNK>\"  # Unknown token (ID: 1) - used for words not in vocabulary\n",
    "\n",
    "def build_vocabulary(df):\n",
    "    \"\"\"\n",
    "    Builds a complete vocabulary from the cleaned log data.\n",
    "    \n",
    "    Purpose: Create a word ‚Üí ID mapping for converting text to numerical format.\n",
    "    Only keep words that appear frequently enough to be meaningful.\n",
    "    \n",
    "    Workflow:\n",
    "        1. Count how many times each word appears in the entire dataset\n",
    "        2. Filter out rare words (noise/typos)\n",
    "        3. Keep only the top N most common words\n",
    "        4. Create bidirectional dictionaries (word‚ÜíID and ID‚Üíword)\n",
    "        5. Save to JSON for reuse\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing cleaned logs\n",
    "    \n",
    "    Returns:\n",
    "        vocab_dict: Dictionary mapping words to IDs\n",
    "    \"\"\"\n",
    "    print(f\"üîé Starting vocabulary construction...\")\n",
    "    \n",
    "    # Step A: Data sampling (optional)\n",
    "    if SAMPLE_SIZE and len(df) > SAMPLE_SIZE:\n",
    "        print(f\"‚ö° Sampling: Using {SAMPLE_SIZE} random rows (faster testing).\")\n",
    "        work_df = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    else:\n",
    "        print(f\"‚ö° FULL MODE: Processing all {len(df):,} rows.\")\n",
    "        work_df = df\n",
    "\n",
    "    # Step B: Count word occurrences\n",
    "    # Counter is a specialized dictionary for counting hashable objects\n",
    "    counter = collections.Counter()\n",
    "    \n",
    "    print(\"üìä Analyzing words in progress...\")\n",
    "    \n",
    "    # Iterate through each log and count words\n",
    "    for text in tqdm(work_df[TEXT_COLUMN], desc=\"Scanning Logs\"):\n",
    "        if isinstance(text, str):\n",
    "            tokens = text.split()  # Split on whitespace\n",
    "            counter.update(tokens)  # Add word counts to counter\n",
    "\n",
    "    print(f\"\\n‚úÖ Analysis complete. Found {len(counter):,} unique words.\")\n",
    "\n",
    "    # Step C: Filtering and Sorting\n",
    "    print(f\"‚úÇÔ∏è Filtering: Keeping words that appear > {MIN_OCCURRENCES} times...\")\n",
    "    \n",
    "    # Get the most common words, limited by VOCAB_SIZE_LIMIT\n",
    "    # Subtract 2 to leave room for PAD and UNK tokens\n",
    "    most_common = counter.most_common(VOCAB_SIZE_LIMIT - 2)\n",
    "    \n",
    "    # Filter out rare words\n",
    "    filtered_vocab = [word for word, count in most_common if count >= MIN_OCCURRENCES]\n",
    "    \n",
    "    print(f\"‚úÖ Final vocabulary: {len(filtered_vocab):,} words.\")\n",
    "\n",
    "    # Step D: Build ID ‚Üî Word dictionaries\n",
    "    # vocab_list = [PAD, UNK, word1, word2, ...]\n",
    "    vocab_list = [PAD_TOKEN, UNK_TOKEN] + filtered_vocab\n",
    "    \n",
    "    # Create word ‚Üí ID mapping (for encoding)\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "    \n",
    "    # Create ID ‚Üí word mapping (for decoding)\n",
    "    inverse_vocab_dict = {idx: word for word, idx in vocab_dict.items()}\n",
    "    \n",
    "    # Step E: Save to JSON for future use\n",
    "    vocab_data = {\n",
    "        \"vocab\": vocab_dict,           # word ‚Üí ID\n",
    "        \"inverse_vocab\": inverse_vocab_dict,  # ID ‚Üí word\n",
    "        \"size\": len(vocab_list),       # Total vocabulary size\n",
    "        \"pad_token\": PAD_TOKEN,        # What token is used for padding\n",
    "        \"unk_token\": UNK_TOKEN,        # What token is used for unknowns\n",
    "        \"min_occurrences\": MIN_OCCURRENCES  # Filtering threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"üíæ Saving to '{OUTPUT_FILE}'...\")\n",
    "    with open(OUTPUT_FILE, \"w\") as f:\n",
    "        json.dump(vocab_data, f, indent=4)\n",
    "        \n",
    "    print(\"‚úÖ Vocabulary saved successfully!\")\n",
    "    \n",
    "    # Print top 10 words (excluding special tokens)\n",
    "    print(\"\\n--- TOP 10 MOST COMMON WORDS ---\")\n",
    "    for word, idx in list(vocab_dict.items())[2:12]:  # Skip PAD and UNK\n",
    "        print(f\"{word:20s} (ID: {idx})\")\n",
    "\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:24,422 - INFO - üìÇ Loading cached vocabulary from checkpoint...\n",
      "2026-01-02 15:32:24,500 - INFO - ‚úÖ Loaded vocabulary from checkpoint (Size: 50000).\n"
     ]
    }
   ],
   "source": [
    "# Load or compute vocabulary\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    logger.info(f\"üìÇ Loading cached vocabulary from checkpoint...\")\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    vocab = vocab_data['vocab']\n",
    "    logger.info(f\"‚úÖ Loaded vocabulary from checkpoint (Size: {vocab_data['size']}).\")\n",
    "else:\n",
    "    logger.info(f\"üîÑ Computing vocabulary from scratch...\")\n",
    "    vocab = build_vocabulary(df)\n",
    "    logger.info(f\"üíæ Saved checkpoint: vocab.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c11a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENCODING CONFIGURATION =====\n",
    "VOCAB_FILE = f\"{CHECKPOINT_DIR}/vocab_full.json\"\n",
    "INPUT_COL = \"cleaned_log\"\n",
    "BATCH_SIZE = 10_000       # Traitement par lot pour ne pas exploser la RAM (r√©duit pour √©viter OOM)\n",
    "MAX_LEN = 128            # Longueur maximale d'un log (tronquer ou pad)\n",
    "\n",
    "def encode_logs(df):\n",
    "    \"\"\"\n",
    "    Convertit la colonne cleaned_log en matrices NumPy (input_ids)\n",
    "    pr√™tes pour PyTorch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Charger le vocabulaire\n",
    "    logger.info(f\"üìÇ Chargement du vocabulaire depuis {VOCAB_FILE}...\")\n",
    "    with open(VOCAB_FILE, 'r') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    \n",
    "    vocab = vocab_data['vocab']\n",
    "    unk_id = vocab[vocab_data['unk_token']] # G√©n√©ralement 1\n",
    "    pad_id = vocab[vocab_data['pad_token']] # G√©n√©ralement 0\n",
    "    \n",
    "    logger.info(f\"‚úÖ Vocabulaire charg√© (Taille: {vocab_data['size']}).\")\n",
    "    logger.info(f\"üîß Configuration: Max_Len={MAX_LEN}, Batch_Size={BATCH_SIZE}\")\n",
    "\n",
    "    all_input_ids = []\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Fonction optimis√©e pour encoder une seule phrase\n",
    "    def text_to_ids(text):\n",
    "        if not isinstance(text, str): return [pad_id]\n",
    "        return [vocab.get(word, unk_id) for word in text.split()]\n",
    "\n",
    "    # Boucle de traitement par Batches\n",
    "    with tqdm(total=total_rows, unit=\"rows\", desc=\"Encodage\") as pbar:\n",
    "        \n",
    "        for i in range(0, total_rows, BATCH_SIZE):\n",
    "            batch_end = min(i + BATCH_SIZE, total_rows)\n",
    "            \n",
    "            # Extraire le batch\n",
    "            chunk_texts = df[INPUT_COL].iloc[i:batch_end]\n",
    "            \n",
    "            # Encoder (Map sur la s√©rie)\n",
    "            encoded_lists = chunk_texts.map(text_to_ids)\n",
    "            \n",
    "            # Padding & Truncation\n",
    "            padded_matrix = np.zeros((len(chunk_texts), MAX_LEN), dtype=np.int32)\n",
    "            \n",
    "            for idx, ids in enumerate(encoded_lists):\n",
    "                seq = ids[:MAX_LEN]\n",
    "                padded_matrix[idx, :len(seq)] = seq\n",
    "            \n",
    "            all_input_ids.append(padded_matrix)\n",
    "            \n",
    "            pbar.update(len(chunk_texts))\n",
    "\n",
    "    # 4. Concat√©nation finale\n",
    "    logger.info(\"üß© Concat√©nation des batches...\")\n",
    "    final_matrix = np.concatenate(all_input_ids, axis=0)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Encodage termin√©. Shape finale: {final_matrix.shape}\")\n",
    "    return final_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96308fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:24,536 - INFO - üìÇ Loading cached encoded logs from checkpoint...\n",
      "2026-01-02 15:32:24,551 - INFO - ‚úÖ Loaded encoded logs from checkpoint. Shape: (246979255, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load or compute encoded logs\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/encoded_logs.npy\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    logger.info(f\"üìÇ Loading cached encoded logs from checkpoint...\")\n",
    "    input_ids_matrix = np.load(checkpoint_path, mmap_mode='r')  # Memory-mapped for large files\n",
    "    logger.info(f\"‚úÖ Loaded encoded logs from checkpoint. Shape: {input_ids_matrix.shape}\")\n",
    "else:\n",
    "    logger.info(f\"üîÑ Computing encoded logs from scratch...\")\n",
    "    \n",
    "    # Process in smaller batches and save directly to disk\n",
    "    temp_file = f\"{CHECKPOINT_DIR}/encoded_logs_temp.npy\"\n",
    "    \n",
    "    # First pass: compute and save batches incrementally\n",
    "    logger.info(f\"üìÇ Chargement du vocabulaire depuis {VOCAB_FILE}...\")\n",
    "    with open(VOCAB_FILE, 'r') as f:\n",
    "        vocab_data_local = json.load(f)\n",
    "    \n",
    "    vocab_local = vocab_data_local['vocab']\n",
    "    unk_id = vocab_local[vocab_data_local['unk_token']]\n",
    "    pad_id = vocab_local[vocab_data_local['pad_token']]\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    ENCODE_BATCH_SIZE = 5000  # Reduced batch size to prevent OOM\n",
    "    \n",
    "    # Create memory-mapped array for incremental writing\n",
    "    input_ids_matrix = np.lib.format.open_memmap(\n",
    "        temp_file, \n",
    "        mode='w+', \n",
    "        dtype=np.int32, \n",
    "        shape=(total_rows, MAX_LEN)\n",
    "    )\n",
    "    \n",
    "    def text_to_ids_local(text):\n",
    "        if not isinstance(text, str): return [pad_id]\n",
    "        return [vocab_local.get(word, unk_id) for word in text.split()]\n",
    "    \n",
    "    with tqdm(total=total_rows, unit=\"rows\", desc=\"Encodage\") as pbar:\n",
    "        for i in range(0, total_rows, ENCODE_BATCH_SIZE):\n",
    "            batch_end = min(i + ENCODE_BATCH_SIZE, total_rows)\n",
    "            chunk_texts = df[INPUT_COL].iloc[i:batch_end]\n",
    "            encoded_lists = chunk_texts.map(text_to_ids_local)\n",
    "            \n",
    "            for idx, ids in enumerate(encoded_lists):\n",
    "                seq = ids[:MAX_LEN]\n",
    "                input_ids_matrix[i + idx, :len(seq)] = seq\n",
    "            \n",
    "            # Force flush to disk periodically\n",
    "            if i % (ENCODE_BATCH_SIZE * 10) == 0:\n",
    "                del input_ids_matrix\n",
    "                gc.collect()\n",
    "                input_ids_matrix = np.lib.format.open_memmap(temp_file, mode='r+', dtype=np.int32)\n",
    "            \n",
    "            pbar.update(batch_end - i)\n",
    "    \n",
    "    # Flush and rename\n",
    "    del input_ids_matrix\n",
    "    gc.collect()\n",
    "    \n",
    "    shutil.move(temp_file, checkpoint_path)\n",
    "    logger.info(f\"üíæ Saved checkpoint: encoded_logs.npy\")\n",
    "    \n",
    "    # Reload as memory-mapped\n",
    "    input_ids_matrix = np.load(checkpoint_path, mmap_mode='r')\n",
    "    logger.info(f\"‚úÖ Encodage termin√©. Shape finale: {input_ids_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fff6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:24,581 - INFO - üîç Checking encoded logs shape and memory usage...\n",
      "2026-01-02 15:32:24,583 - INFO - Shape: (246979255, 128)\n",
      "2026-01-02 15:32:24,584 - INFO - Dtype: int32\n",
      "2026-01-02 15:32:24,584 - INFO - Memory usage: 126.45 GB\n",
      "2026-01-02 15:32:24,584 - INFO - Total elements: 31,613,344,640\n",
      "2026-01-02 15:32:24,584 - WARNING - ‚ö†Ô∏è DATASET TOO LARGE! Using only first 500,000 samples\n",
      "2026-01-02 15:32:24,584 - INFO - Original: 246,979,255 samples ‚Üí Now: 500,000 samples\n",
      "2026-01-02 15:32:24,584 - INFO - New shape: (500000, 128)\n"
     ]
    }
   ],
   "source": [
    "# ===== CHECK ENCODED DATA SIZE =====\n",
    "logger.info(f\"üîç Checking encoded logs shape and memory usage...\")\n",
    "logger.info(f\"Shape: {input_ids_matrix.shape}\")\n",
    "logger.info(f\"Dtype: {input_ids_matrix.dtype}\")\n",
    "logger.info(f\"Memory usage: {input_ids_matrix.nbytes / 1e9:.2f} GB\")\n",
    "logger.info(f\"Total elements: {input_ids_matrix.size:,}\")\n",
    "\n",
    "# If too large, use only first N samples\n",
    "MAX_SAMPLES_FOR_TRAINING = 500_000  # Reduced from 2M - batch 64 requires ~30 min per epoch otherwise\n",
    "\n",
    "if len(input_ids_matrix) > MAX_SAMPLES_FOR_TRAINING:\n",
    "    logger.warning(f\"‚ö†Ô∏è DATASET TOO LARGE! Using only first {MAX_SAMPLES_FOR_TRAINING:,} samples\")\n",
    "    logger.info(f\"Original: {len(input_ids_matrix):,} samples ‚Üí Now: {MAX_SAMPLES_FOR_TRAINING:,} samples\")\n",
    "    input_ids_matrix = input_ids_matrix[:MAX_SAMPLES_FOR_TRAINING]\n",
    "    logger.info(f\"New shape: {input_ids_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1bca6",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Now that we have encoded logs, we'll train an **Autoencoder** to learn the normal structure of logs.\n",
    "\n",
    "The model will:\n",
    "1. **Compress** each log into a latent representation (via LSTM Encoder)\n",
    "2. **Reconstruct** it back to the original vocabulary distribution (Decoder)\n",
    "3. Learn by minimizing reconstruction error\n",
    "\n",
    "After training, normal logs will have low reconstruction error, while anomalies will have high error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677223d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m.cuda.is_available():\n\u001b[32m      2\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m      3\u001b[39m     torch.cuda.reset_peak_memory_stats()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    logger.info(f\"üßπ GPU cache cleared before model initialization\")\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39b31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:24,859 - INFO - üßπ GPU cache cleared before model initialization\n",
      "2026-01-02 15:32:24,964 - INFO - ü§ñ Initializing LogAutoEncoder...\n",
      "2026-01-02 15:32:25,299 - INFO - üìà Total parameters: 4,871,312\n",
      "2026-01-02 15:32:25,302 - INFO - üìà Trainable parameters: 4,871,312\n",
      "2026-01-02 15:32:26,899 - INFO - ‚úÖ Model ready for training on cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== DEFINE AUTOENCODER MODEL =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. D√©finir PAD_IDX (Id√©alement 0 comme dans l'embedding)\n",
    "PAD_IDX = 0 \n",
    "VOCAB_SIZE = vocab_data['size']\n",
    "\n",
    "class LogAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=48, latent_dim=24):\n",
    "        super(LogAutoEncoder, self).__init__()\n",
    "        \n",
    "        # 1. Couche d'Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # 2. Encodeur LSTM\n",
    "        # On compresse la s√©quence vers latent_dim (ex: 24)\n",
    "        self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "        \n",
    "        # 3. D√©codeur LSTM\n",
    "        # Input size = latent_dim (what we feed it)\n",
    "        # Hidden size = embed_dim (internal representation)\n",
    "        self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "        \n",
    "        # 4. Couche de sortie finale\n",
    "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.max_len = 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x) # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # --- ENCODAGE ---\n",
    "        _, (h_n, c_n) = self.encoder(embedded)\n",
    "        # h_n shape: [1, batch, latent_dim]\n",
    "        latent = h_n.squeeze(0) # [batch, latent_dim]\n",
    "        \n",
    "        # --- D√âCODAGE ---\n",
    "        # On doit r√©p√©ter le vecteur latent pour chaque pas de temps\n",
    "        latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1) # [batch, seq_len, latent_dim]\n",
    "        \n",
    "        # On passe au d√©codeur sans √©tat cach√© (il en cr√©e un nouveau)\n",
    "        decoded, _ = self.decoder(latent_repeated)\n",
    "        \n",
    "        # Projection finale vers le vocabulaire\n",
    "        reconstruction = self.output_linear(decoded) # [batch, seq_len, vocab_size]\n",
    "        \n",
    "        return reconstruction, latent\n",
    "\n",
    "# Clear GPU cache and CPU memory before model init\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    logger.info(f\"üßπ GPU cache cleared before model initialization\")\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Initialize model\n",
    "logger.info(f\"ü§ñ Initializing LogAutoEncoder...\")\n",
    "model = LogAutoEncoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=48,  # Reduced from 64 for faster training\n",
    "    latent_dim=24  # Reduced from 32 for faster training\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"üìà Total parameters: {total_params:,}\")\n",
    "logger.info(f\"üìà Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "# Optimizer with higher learning rate for faster convergence\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002, weight_decay=1e-5)\n",
    "logger.info(f\"‚úÖ Model ready for training on {DEVICE}\")\n",
    "\n",
    "# Loss function - CrossEntropy (standard for reconstruction)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2e78ed",
   "metadata": {},
   "source": [
    "## 4.1 Training Parameters Optimization for RTX 4090\n",
    "\n",
    "For faster training on RTX 4090 (24GB VRAM) - Complete in ~2 hours\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb6741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:26,917 - INFO - \n",
      "======================================================================\n",
      "2026-01-02 15:32:26,917 - INFO - ‚ö° RTX 4090 OPTIMIZED TRAINING CONFIGURATION\n",
      "2026-01-02 15:32:26,917 - INFO - ======================================================================\n",
      "2026-01-02 15:32:26,917 - INFO - GPU: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "2026-01-02 15:32:26,917 - INFO - VRAM: 17.2 GB\n",
      "2026-01-02 15:32:26,917 - WARNING - ‚ö†Ô∏è RTX 3090/4080 Detected (16GB). Using moderate optimization.\n"
     ]
    }
   ],
   "source": [
    "# ===== OPTIMIZED PARAMETERS FOR RTX 4090 (24GB VRAM) =====\n",
    "# Target: Complete training in ~2 hours\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"‚ö° RTX 4090 OPTIMIZED TRAINING CONFIGURATION\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "# Check GPU capabilities\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"GPU: {gpu_name}\")\n",
    "    logger.info(f\"VRAM: {total_vram:.1f} GB\")\n",
    "    \n",
    "    if total_vram >= 24:\n",
    "        logger.info(\"‚úÖ RTX 4090 Detected! Using aggressive optimization.\")\n",
    "        # RTX 4090 (24GB) Optimized Settings\n",
    "        EPOCHS_RTX4090 = 10              # Fewer epochs but larger batches\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 1024  # Much larger batch (RTX 4090 can handle)\n",
    "        LEARNING_RATE_RTX4090 = 0.004    # Higher LR for faster convergence\n",
    "        USE_MIXED_PRECISION_RTX4090 = True\n",
    "        GRADIENT_ACCUMULATION_STEPS_RTX4090 = 1\n",
    "        NUM_WORKERS_RTX4090 = 2          # 4 workers for parallel data loading (~45min total vs 2.3h with 0)\n",
    "        \n",
    "        logger.info(f\"\\nüî• Optimized Parameters:\")\n",
    "        logger.info(f\"   Epochs: {EPOCHS_RTX4090}\")\n",
    "        logger.info(f\"   Batch Size: {TRAIN_BATCH_SIZE_RTX4090}\")\n",
    "        logger.info(f\"   Learning Rate: {LEARNING_RATE_RTX4090}\")\n",
    "        logger.info(f\"   Mixed Precision: {USE_MIXED_PRECISION_RTX4090}\")\n",
    "        logger.info(f\"   Num Workers: {NUM_WORKERS_RTX4090}\")\n",
    "        \n",
    "        # Calculate estimated time\n",
    "        num_batches_per_epoch = len(dataset) // TRAIN_BATCH_SIZE_RTX4090\n",
    "        time_per_batch_ms = 10  # RTX 4090 @ ~100 it/s\n",
    "        time_per_epoch_min = (num_batches_per_epoch * time_per_batch_ms) / 60000\n",
    "        total_time_min = time_per_epoch_min * EPOCHS_RTX4090\n",
    "        \n",
    "        logger.info(f\"\\n‚è±Ô∏è  Estimated Training Time:\")\n",
    "        logger.info(f\"   Per epoch: ~{time_per_epoch_min:.0f} minutes\")\n",
    "        logger.info(f\"   Total ({EPOCHS_RTX4090} epochs): ~{total_time_min:.0f} minutes (~{total_time_min/60:.1f} hours)\")\n",
    "        logger.info(f\"   Should complete in: < 2 hours ‚úÖ\")\n",
    "        \n",
    "    elif total_vram >= 16:\n",
    "        logger.warning(\"‚ö†Ô∏è RTX 3090/4080 Detected (16GB). Using moderate optimization.\")\n",
    "        EPOCHS_RTX4090 = 12\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 512\n",
    "        LEARNING_RATE_RTX4090 = 0.003\n",
    "        USE_MIXED_PRECISION_RTX4090 = True\n",
    "        GRADIENT_ACCUMULATION_STEPS_RTX4090 = 1\n",
    "        NUM_WORKERS_RTX4090 = 2\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Limited VRAM detected. Using conservative settings.\")\n",
    "        EPOCHS_RTX4090 = 15\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 256\n",
    "        LEARNING_RATE_RTX4090 = 0.002\n",
    "        USE_MIXED_PRECISION_RTX4090 = True\n",
    "        GRADIENT_ACCUMULATION_STEPS_RTX4090 = 1\n",
    "        NUM_WORKERS_RTX4090 = 0\n",
    "\n",
    "else:\n",
    "    logger.warning(\"‚ùå No GPU detected. Training will be slow!\")\n",
    "    EPOCHS_RTX4090 = 15\n",
    "    TRAIN_BATCH_SIZE_RTX4090 = 64\n",
    "    LEARNING_RATE_RTX4090 = 0.002\n",
    "    USE_MIXED_PRECISION_RTX4090 = False\n",
    "    GRADIENT_ACCUMULATION_STEPS_RTX4090 = 2\n",
    "    NUM_WORKERS_RTX4090 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da811075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:32:26,950 - INFO - \n",
      "======================================================================\n",
      "2026-01-02 15:32:26,950 - INFO - ‚úÖ CHECKING PREREQUISITES FOR TRAINING\n",
      "2026-01-02 15:32:26,950 - INFO - ======================================================================\n",
      "2026-01-02 15:32:26,952 - WARNING - ‚ö†Ô∏è Dataset not found! Creating it from encoded logs...\n",
      "2026-01-02 15:32:26,954 - INFO - ‚úÖ Created dataset with 500,000 samples\n",
      "2026-01-02 15:32:26,956 - INFO - ‚úÖ Model already loaded\n",
      "2026-01-02 15:32:26,958 - INFO - ‚úÖ Optimizer already exists\n",
      "2026-01-02 15:32:26,959 - INFO - ‚úÖ Criterion already exists\n",
      "2026-01-02 15:32:26,959 - WARNING - ‚ö†Ô∏è Scheduler not found! Creating CosineAnnealingLR...\n",
      "2026-01-02 15:32:26,959 - INFO - ‚úÖ Created CosineAnnealingLR scheduler\n",
      "2026-01-02 15:32:26,961 - INFO - \n",
      "‚ú® All prerequisites satisfied! Ready to train.\n",
      "2026-01-02 15:32:26,961 - INFO - üìä Dataset size: 500,000\n",
      "2026-01-02 15:32:26,963 - INFO - ü§ñ Model parameters: 4,871,312\n",
      "2026-01-02 15:32:26,963 - INFO - üîß Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== PREREQUISITE CHECK: ENSURE ALL REQUIRED VARIABLES EXIST =====\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"‚úÖ CHECKING PREREQUISITES FOR TRAINING\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "# Check if dataset exists, if not create it\n",
    "if 'dataset' not in locals() or 'dataset' not in globals():\n",
    "    logger.warning(\"‚ö†Ô∏è Dataset not found! Creating it from encoded logs...\")\n",
    "    \n",
    "    # Need to ensure LogDataset class is defined\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    class LogDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return torch.tensor(self.data[idx], dtype=torch.long)\n",
    "    \n",
    "    dataset = LogDataset(input_ids_matrix)\n",
    "    logger.info(f\"‚úÖ Created dataset with {len(dataset):,} samples\")\n",
    "else:\n",
    "    logger.info(f\"‚úÖ Dataset already exists: {len(dataset):,} samples\")\n",
    "\n",
    "# Check if model exists\n",
    "if 'model' not in locals() or 'model' not in globals():\n",
    "    logger.warning(\"‚ö†Ô∏è Model not found! Creating a new one...\")\n",
    "    \n",
    "    # Define the model class\n",
    "    class LogAutoEncoder(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_dim=48, latent_dim=24):\n",
    "            super(LogAutoEncoder, self).__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "            self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "            self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "            self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "            self.max_len = 128\n",
    "\n",
    "        def forward(self, x):\n",
    "            embedded = self.embedding(x)\n",
    "            _, (h_n, c_n) = self.encoder(embedded)\n",
    "            latent = h_n.squeeze(0)\n",
    "            latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1)\n",
    "            decoded, _ = self.decoder(latent_repeated)\n",
    "            reconstruction = self.output_linear(decoded)\n",
    "            return reconstruction, latent\n",
    "    \n",
    "    # Create new model\n",
    "    model = LogAutoEncoder(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=48,\n",
    "        latent_dim=24\n",
    "    ).to(DEVICE)\n",
    "    logger.info(f\"‚úÖ Created new model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "else:\n",
    "    logger.info(f\"‚úÖ Model already loaded\")\n",
    "\n",
    "# Check if optimizer exists\n",
    "if 'optimizer' not in locals() or 'optimizer' not in globals():\n",
    "    logger.warning(\"‚ö†Ô∏è Optimizer not found! Creating a new one...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_RTX4090, weight_decay=1e-5)\n",
    "    logger.info(f\"‚úÖ Created optimizer with LR={LEARNING_RATE_RTX4090}\")\n",
    "else:\n",
    "    logger.info(f\"‚úÖ Optimizer already exists\")\n",
    "\n",
    "# Check if criterion exists\n",
    "if 'criterion' not in locals() or 'criterion' not in globals():\n",
    "    logger.warning(\"‚ö†Ô∏è Loss function not found! Creating CrossEntropyLoss...\")\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    logger.info(f\"‚úÖ Created CrossEntropyLoss criterion\")\n",
    "else:\n",
    "    logger.info(f\"‚úÖ Criterion already exists\")\n",
    "\n",
    "# Check if scheduler exists (optional but recommended)\n",
    "if 'scheduler' not in locals() or 'scheduler' not in globals():\n",
    "    logger.warning(\"‚ö†Ô∏è Scheduler not found! Creating CosineAnnealingLR...\")\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_RTX4090, eta_min=1e-6)\n",
    "    logger.info(f\"‚úÖ Created CosineAnnealingLR scheduler\")\n",
    "else:\n",
    "    logger.info(f\"‚úÖ Scheduler already exists\")\n",
    "\n",
    "logger.info(f\"\\n‚ú® All prerequisites satisfied! Ready to train.\")\n",
    "logger.info(f\"üìä Dataset size: {len(dataset):,}\")\n",
    "logger.info(f\"ü§ñ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "logger.info(f\"üîß Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dc2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ===== OPTIMIZED TRAINING WITH RTX 4090 SETTINGS =====\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport os\\nimport logging\\nimport time\\nfrom tqdm.auto import tqdm\\nfrom torch.amp import autocast, GradScaler\\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\\n\\n# Use optimized parameters\\nEPOCHS = EPOCHS_RTX4090\\nTRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE_RTX4090\\nLEARNING_RATE = LEARNING_RATE_RTX4090\\nUSE_MIXED_PRECISION = USE_MIXED_PRECISION_RTX4090\\nGRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS_RTX4090\\n\\nlogger.info(f\"\\nüöÄ STARTING OPTIMIZED TRAINING\")\\nlogger.info(f\"=\"*70)\\n\\n# Create optimized dataloader\\ndataloader_optimized = DataLoader(\\n    dataset, \\n    batch_size=TRAIN_BATCH_SIZE, \\n    shuffle=True, \\n    num_workers=NUM_WORKERS_RTX4090,\\n    pin_memory=True,\\n    persistent_workers=True if NUM_WORKERS_RTX4090 > 0 else False\\n)\\n\\nlogger.info(f\"üìä Dataset: {len(dataset):,} samples\")\\nlogger.info(f\"üì¶ Batch Size: {TRAIN_BATCH_SIZE}\")\\nlogger.info(f\"üìà Batches per epoch: {len(dataloader_optimized)}\")\\nlogger.info(f\"‚è±Ô∏è  Estimated epoch time: ~{len(dataloader_optimized) / 100:.1f} minutes\")\\nlogger.info(f\"üîã Total estimated time: ~{(len(dataloader_optimized) / 100) * EPOCHS / 60:.1f} hours\")\\n\\n# Reinitialize optimizer with new learning rate\\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\\nscheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\\n\\n# GPU optimization flags\\nif torch.cuda.is_available():\\n    torch.cuda.empty_cache()\\n    torch.cuda.reset_peak_memory_stats()\\n    torch.cuda.synchronize()  # Sync before timing\\n\\n    # Enable cuDNN auto-tuner for maximum performance\\n    torch.backends.cudnn.benchmark = True\\n    torch.backends.cudnn.deterministic = False\\n\\n    logger.info(f\"üî• GPU Optimizations Enabled:\")\\n    logger.info(f\"   cuDNN Benchmark: True\")\\n    logger.info(f\"   TF32: True\")\\n\\nscaler = GradScaler(\"cuda\") if USE_MIXED_PRECISION else None\\n\\ntrain_losses = []\\nstart_time = time.time()\\n\\nfor epoch in range(EPOCHS):\\n    model.train()\\n    total_loss = 0.0\\n    num_batches = 0\\n    epoch_start = time.time()\\n\\n    if torch.cuda.is_available():\\n        torch.cuda.reset_peak_memory_stats()\\n\\n    pbar = tqdm(\\n        enumerate(dataloader_optimized),\\n        total=len(dataloader_optimized),\\n        desc=f\"Epoch {epoch+1}/{EPOCHS}\",\\n        leave=True,\\n        ncols=120  # Wider progress bar\\n    )\\n\\n    for batch_idx, batch_data in pbar:\\n        batch_data = batch_data.to(DEVICE, non_blocking=True)\\n\\n        if USE_MIXED_PRECISION:\\n            with autocast(device_type=\"cuda\", dtype=torch.float16):\\n                reconstruction, latent = model(batch_data)\\n                batch_size, seq_len, vocab_size = reconstruction.shape\\n\\n                reconstruction_flat = reconstruction.view(-1, vocab_size)\\n                target_flat = batch_data.view(-1)\\n                loss = criterion(reconstruction_flat, target_flat)\\n\\n                loss = loss / GRADIENT_ACCUMULATION_STEPS\\n\\n            scaler.scale(loss).backward()\\n        else:\\n            optimizer.zero_grad()\\n            reconstruction, latent = model(batch_data)\\n            batch_size, seq_len, vocab_size = reconstruction.shape\\n\\n            reconstruction_flat = reconstruction.view(-1, vocab_size)\\n            target_flat = batch_data.view(-1)\\n            loss = criterion(reconstruction_flat, target_flat)\\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\\n            loss.backward()\\n\\n        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\\n            if USE_MIXED_PRECISION:\\n                scaler.unscale_(optimizer)\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n\\n            if USE_MIXED_PRECISION:\\n                scaler.step(optimizer)\\n                scaler.update()\\n            else:\\n                optimizer.step()\\n\\n            optimizer.zero_grad()\\n\\n        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\\n        num_batches += 1\\n\\n        # Get GPU memory info\\n        gpu_mem_used = 0\\n        gpu_mem_total = 0\\n        if torch.cuda.is_available():\\n            gpu_mem_used = torch.cuda.memory_allocated(0) / 1e9\\n            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\\n\\n        # Update progress bar with detailed info\\n        pbar.set_postfix({\\n            \\'loss\\': f\\'{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\\',\\n            \\'lr\\': f\\'{optimizer.param_groups[0][\"lr\"]:.6f}\\',\\n            f\\'GPU\\': f\\'{gpu_mem_used:.1f}/{gpu_mem_total:.1f}GB\\',\\n            \\'speed\\': f\\'{pbar.format_dict[\"rate\"]:.1f}it/s\\' if pbar.format_dict[\"rate\"] else \\'N/A\\'\\n        })\\n\\n    scheduler.step()\\n\\n    avg_loss = total_loss / num_batches\\n    train_losses.append(avg_loss)\\n\\n    epoch_time = time.time() - epoch_start\\n    current_lr = optimizer.param_groups[0][\\'lr\\']\\n\\n    logger.info(f\"‚úÖ Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\\n\\n    # Save checkpoint\\n    if (epoch + 1) % 1 == 0:  # Save every epoch\\n        checkpoint_path = f\"{CHECKPOINT_DIR}/autoencoder_rtx4090_epoch_{epoch+1}.pth\"\\n\\n        torch.save({\\n            \\'epoch\\': epoch + 1,\\n            \\'model_state_dict\\': model.state_dict(),\\n            \\'optimizer_state_dict\\': optimizer.state_dict(),\\n            \\'scheduler_state_dict\\': scheduler.state_dict(),\\n            \\'loss\\': avg_loss,\\n            \\'vocab_size\\': VOCAB_SIZE,\\n            \\'embed_dim\\': 48,\\n            \\'latent_dim\\': 24,\\n            \\'max_len\\': MAX_LEN,\\n        }, checkpoint_path)\\n        logger.info(f\"üíæ Saved: {checkpoint_path}\")\\n\\ntotal_time = time.time() - start_time\\nlogger.info(f\"\\nüéâ TRAINING COMPLETE!\")\\nlogger.info(f\"=\"*70)\\nlogger.info(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\\nlogger.info(f\"üìä Final loss: {train_losses[-1]:.4f}\")\\nif torch.cuda.is_available():\\n    logger.info(f\"üñ•Ô∏è Peak GPU Memory: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# ===== OPTIMIZED TRAINING WITH RTX 4090 SETTINGS =====\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Use optimized parameters\n",
    "EPOCHS = EPOCHS_RTX4090\n",
    "TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE_RTX4090\n",
    "LEARNING_RATE = LEARNING_RATE_RTX4090\n",
    "USE_MIXED_PRECISION = USE_MIXED_PRECISION_RTX4090\n",
    "GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS_RTX4090\n",
    "\n",
    "logger.info(f\"\\nüöÄ STARTING OPTIMIZED TRAINING\")\n",
    "logger.info(f\"=\"*70)\n",
    "\n",
    "# Create optimized dataloader\n",
    "dataloader_optimized = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS_RTX4090,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS_RTX4090 > 0 else False\n",
    ")\n",
    "\n",
    "logger.info(f\"üìä Dataset: {len(dataset):,} samples\")\n",
    "logger.info(f\"üì¶ Batch Size: {TRAIN_BATCH_SIZE}\")\n",
    "logger.info(f\"üìà Batches per epoch: {len(dataloader_optimized)}\")\n",
    "logger.info(f\"‚è±Ô∏è  Estimated epoch time: ~{len(dataloader_optimized) / 100:.1f} minutes\")\n",
    "logger.info(f\"üîã Total estimated time: ~{(len(dataloader_optimized) / 100) * EPOCHS / 60:.1f} hours\")\n",
    "\n",
    "# Reinitialize optimizer with new learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# GPU optimization flags\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()  # Sync before timing\n",
    "    \n",
    "    # Enable cuDNN auto-tuner for maximum performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    logger.info(f\"üî• GPU Optimizations Enabled:\")\n",
    "    logger.info(f\"   cuDNN Benchmark: True\")\n",
    "    logger.info(f\"   TF32: True\")\n",
    "\n",
    "scaler = GradScaler(\"cuda\") if USE_MIXED_PRECISION else None\n",
    "\n",
    "train_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader_optimized),\n",
    "        total=len(dataloader_optimized),\n",
    "        desc=f\"Epoch {epoch+1}/{EPOCHS}\",\n",
    "        leave=True,\n",
    "        ncols=120  # Wider progress bar\n",
    "    )\n",
    "    \n",
    "    for batch_idx, batch_data in pbar:\n",
    "        batch_data = batch_data.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        if USE_MIXED_PRECISION:\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                reconstruction, latent = model(batch_data)\n",
    "                batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "                \n",
    "                reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "                target_flat = batch_data.view(-1)\n",
    "                loss = criterion(reconstruction_flat, target_flat)\n",
    "                \n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, latent = model(batch_data)\n",
    "            batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "            \n",
    "            reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "            target_flat = batch_data.view(-1)\n",
    "            loss = criterion(reconstruction_flat, target_flat)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if USE_MIXED_PRECISION:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            if USE_MIXED_PRECISION:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Get GPU memory info\n",
    "        gpu_mem_used = 0\n",
    "        gpu_mem_total = 0\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Update progress bar with detailed info\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}',\n",
    "            f'GPU': f'{gpu_mem_used:.1f}/{gpu_mem_total:.1f}GB',\n",
    "            'speed': f'{pbar.format_dict[\"rate\"]:.1f}it/s' if pbar.format_dict[\"rate\"] else 'N/A'\n",
    "        })\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    logger.info(f\"‚úÖ Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 1 == 0:  # Save every epoch\n",
    "        checkpoint_path = f\"{CHECKPOINT_DIR}/autoencoder_rtx4090_epoch_{epoch+1}.pth\"\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'vocab_size': VOCAB_SIZE,\n",
    "            'embed_dim': 48,\n",
    "            'latent_dim': 24,\n",
    "            'max_len': MAX_LEN,\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f\"üíæ Saved: {checkpoint_path}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "logger.info(f\"\\nüéâ TRAINING COMPLETE!\")\n",
    "logger.info(f\"=\"*70)\n",
    "logger.info(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "logger.info(f\"üìä Final loss: {train_losses[-1]:.4f}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"üñ•Ô∏è Peak GPU Memory: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e90472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 16:37:45,810 - INFO - \n",
      "================================================================================\n",
      "2026-01-02 16:37:45,812 - INFO - üì¶ LOADING ALL TRAINING VARIABLES & DATA\n",
      "2026-01-02 16:37:45,813 - INFO - ================================================================================\n",
      "2026-01-02 16:37:45,814 - INFO - üìÅ Checkpoint directory: ./checkpoints\n",
      "2026-01-02 16:37:46,266 - INFO - üñ•Ô∏è  Device: cuda\n",
      "2026-01-02 16:37:46,267 - INFO - \n",
      "üìö Loading vocabulary from ./checkpoints/vocab_full.json...\n",
      "2026-01-02 16:37:46,318 - INFO - ‚úÖ Vocabulary loaded: 50,000 tokens\n",
      "2026-01-02 16:37:46,318 - INFO -    PAD token: '<PAD>' (ID: 0)\n",
      "2026-01-02 16:37:46,319 - INFO -    UNK token: '<UNK>' (ID: 1)\n",
      "2026-01-02 16:37:46,319 - INFO - \n",
      "üìä Loading encoded logs from ./checkpoints/encoded_logs.npy...\n",
      "2026-01-02 16:37:46,322 - INFO - ‚úÖ Encoded logs loaded (memory-mapped): (246979255, 128)\n",
      "2026-01-02 16:37:46,323 - INFO -    Dtype: int32\n",
      "2026-01-02 16:37:46,323 - INFO -    Memory: 126.45 GB\n",
      "2026-01-02 16:37:46,324 - INFO - \n",
      "üìÑ Loading cleaned logs from ./checkpoints/temp_cleaned.csv...\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD ALL REQUIRED VARIABLES FOR TRAINING =====\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"üì¶ LOADING ALL TRAINING VARIABLES & DATA\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# ===== CHECKPOINT & DATA PATHS =====\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "VOCAB_FILE = f\"{CHECKPOINT_DIR}/vocab_full.json\"\n",
    "ENCODED_LOGS_FILE = f\"{CHECKPOINT_DIR}/encoded_logs.npy\"\n",
    "CLEANED_DATA_FILE = f\"{CHECKPOINT_DIR}/temp_cleaned.csv\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "logger.info(f\"üìÅ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# ===== DEVICE CONFIGURATION =====\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"üñ•Ô∏è  Device: {DEVICE}\")\n",
    "\n",
    "# ===== LOAD VOCABULARY =====\n",
    "logger.info(f\"\\nüìö Loading vocabulary from {VOCAB_FILE}...\")\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå Vocabulary file not found: {VOCAB_FILE}\\n\"\n",
    "                          \"Please run the vocabulary building cell first!\")\n",
    "\n",
    "with open(VOCAB_FILE, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab_dict = vocab_data[\"vocab\"]\n",
    "inverse_vocab = vocab_data[\"inverse_vocab\"]\n",
    "VOCAB_SIZE = vocab_data[\"size\"]\n",
    "PAD_TOKEN = vocab_data[\"pad_token\"]\n",
    "UNK_TOKEN = vocab_data[\"unk_token\"]\n",
    "\n",
    "pad_id = vocab_dict[PAD_TOKEN]\n",
    "unk_id = vocab_dict[UNK_TOKEN]\n",
    "\n",
    "logger.info(f\"‚úÖ Vocabulary loaded: {VOCAB_SIZE:,} tokens\")\n",
    "logger.info(f\"   PAD token: '{PAD_TOKEN}' (ID: {pad_id})\")\n",
    "logger.info(f\"   UNK token: '{UNK_TOKEN}' (ID: {unk_id})\")\n",
    "\n",
    "# ===== LOAD ENCODED LOGS =====\n",
    "logger.info(f\"\\nüìä Loading encoded logs from {ENCODED_LOGS_FILE}...\")\n",
    "if not os.path.exists(ENCODED_LOGS_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå Encoded logs file not found: {ENCODED_LOGS_FILE}\\n\"\n",
    "                          \"Please run the encoding cell first!\")\n",
    "\n",
    "# Memory-map large file to avoid loading entire array into RAM\n",
    "input_ids_matrix = np.load(ENCODED_LOGS_FILE, mmap_mode='r')\n",
    "logger.info(f\"‚úÖ Encoded logs loaded (memory-mapped): {input_ids_matrix.shape}\")\n",
    "logger.info(f\"   Dtype: {input_ids_matrix.dtype}\")\n",
    "logger.info(f\"   Memory: {input_ids_matrix.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "# ===== LOAD CLEANED DATA (for reference) =====\n",
    "logger.info(f\"\\nüìÑ Loading cleaned logs from {CLEANED_DATA_FILE}...\")\n",
    "if os.path.exists(CLEANED_DATA_FILE):\n",
    "    try:\n",
    "        df_cleaned = pd.read_csv(CLEANED_DATA_FILE)\n",
    "        logger.info(f\"‚úÖ Cleaned data loaded: {len(df_cleaned):,} rows\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Could not load cleaned data: {e}\")\n",
    "        df_cleaned = None\n",
    "else:\n",
    "    logger.warning(f\"‚ö†Ô∏è Cleaned data file not found: {CLEANED_DATA_FILE}\")\n",
    "    df_cleaned = None\n",
    "\n",
    "# ===== MODEL CONFIGURATION =====\n",
    "MAX_LEN = 128\n",
    "EMBED_DIM = 48\n",
    "LATENT_DIM = 24\n",
    "\n",
    "logger.info(f\"\\nü§ñ Model configuration:\")\n",
    "logger.info(f\"   Vocab Size: {VOCAB_SIZE}\")\n",
    "logger.info(f\"   Max Sequence Length: {MAX_LEN}\")\n",
    "logger.info(f\"   Embedding Dimension: {EMBED_DIM}\")\n",
    "logger.info(f\"   Latent Dimension: {LATENT_DIM}\")\n",
    "\n",
    "# ===== RTX 4090 OPTIMIZED HYPERPARAMETERS =====\n",
    "# Auto-detect GPU capabilities\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"\\nüî• GPU Detected: {gpu_name} ({total_vram:.1f} GB VRAM)\")\n",
    "    \n",
    "    if total_vram >= 24:\n",
    "        EPOCHS_RTX4090 = 10\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 1024\n",
    "        LEARNING_RATE_RTX4090 = 0.004\n",
    "        NUM_WORKERS_RTX4090 = 2\n",
    "    elif total_vram >= 16:\n",
    "        EPOCHS_RTX4090 = 12\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 512\n",
    "        LEARNING_RATE_RTX4090 = 0.003\n",
    "        NUM_WORKERS_RTX4090 = 2\n",
    "    else:\n",
    "        EPOCHS_RTX4090 = 15\n",
    "        TRAIN_BATCH_SIZE_RTX4090 = 256\n",
    "        LEARNING_RATE_RTX4090 = 0.002\n",
    "        NUM_WORKERS_RTX4090 = 0\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è No GPU detected! Using CPU (training will be slow)\")\n",
    "    EPOCHS_RTX4090 = 15\n",
    "    TRAIN_BATCH_SIZE_RTX4090 = 64\n",
    "    LEARNING_RATE_RTX4090 = 0.002\n",
    "    NUM_WORKERS_RTX4090 = 0\n",
    "\n",
    "USE_MIXED_PRECISION_RTX4090 = True if torch.cuda.is_available() else False\n",
    "GRADIENT_ACCUMULATION_STEPS_RTX4090 = 1\n",
    "\n",
    "logger.info(f\"‚ö° Training Configuration:\")\n",
    "logger.info(f\"   Epochs: {EPOCHS_RTX4090}\")\n",
    "logger.info(f\"   Batch Size: {TRAIN_BATCH_SIZE_RTX4090}\")\n",
    "logger.info(f\"   Learning Rate: {LEARNING_RATE_RTX4090}\")\n",
    "logger.info(f\"   Mixed Precision: {USE_MIXED_PRECISION_RTX4090}\")\n",
    "logger.info(f\"   Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS_RTX4090}\")\n",
    "logger.info(f\"   Num Workers: {NUM_WORKERS_RTX4090}\")\n",
    "\n",
    "# ===== CREATE DATASET =====\n",
    "logger.info(f\"\\nüì¶ Creating dataset...\")\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, data, max_len=MAX_LEN):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        # Ensure sequence is proper length\n",
    "        if len(seq) < self.max_len:\n",
    "            seq = np.pad(seq, (0, self.max_len - len(seq)), constant_values=pad_id)\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "dataset = LogDataset(input_ids_matrix)\n",
    "logger.info(f\"‚úÖ Dataset created: {len(dataset):,} samples\")\n",
    "\n",
    "# ===== CREATE OPTIMIZED DATALOADER =====\n",
    "dataloader_optimized = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE_RTX4090,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS_RTX4090,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True if NUM_WORKERS_RTX4090 > 0 else False\n",
    ")\n",
    "\n",
    "logger.info(f\"‚úÖ DataLoader created:\")\n",
    "logger.info(f\"   Batch Size: {TRAIN_BATCH_SIZE_RTX4090}\")\n",
    "logger.info(f\"   Batches per Epoch: {len(dataloader_optimized)}\")\n",
    "logger.info(f\"   Num Workers: {NUM_WORKERS_RTX4090}\")\n",
    "\n",
    "# ===== CREATE MODEL =====\n",
    "logger.info(f\"\\nü§ñ Creating model...\")\n",
    "\n",
    "class LogAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=EMBED_DIM, latent_dim=LATENT_DIM, max_len=MAX_LEN):\n",
    "        super(LogAutoEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "        self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.encoder(embedded)\n",
    "        latent = h_n.squeeze(0)\n",
    "        latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1)\n",
    "        decoded, _ = self.decoder(latent_repeated)\n",
    "        reconstruction = self.output_linear(decoded)\n",
    "        return reconstruction, latent\n",
    "\n",
    "model = LogAutoEncoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    max_len=MAX_LEN\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"‚úÖ Model created: {total_params:,} parameters\")\n",
    "\n",
    "# ===== CREATE LOSS FUNCTION =====\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "logger.info(f\"‚úÖ Loss function: CrossEntropyLoss (ignore_index={pad_id})\")\n",
    "\n",
    "# ===== CREATE OPTIMIZER =====\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_RTX4090, weight_decay=1e-5)\n",
    "logger.info(f\"‚úÖ Optimizer: Adam (lr={LEARNING_RATE_RTX4090})\")\n",
    "\n",
    "# ===== CREATE SCHEDULER =====\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS_RTX4090, eta_min=1e-6)\n",
    "logger.info(f\"‚úÖ Scheduler: CosineAnnealingLR (T_max={EPOCHS_RTX4090})\")\n",
    "\n",
    "# ===== MIXED PRECISION SETUP =====\n",
    "if USE_MIXED_PRECISION_RTX4090:\n",
    "    scaler = GradScaler(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"‚úÖ Mixed Precision: Enabled with GradScaler\")\n",
    "else:\n",
    "    scaler = None\n",
    "    logger.info(f\"‚ö™ Mixed Precision: Disabled\")\n",
    "\n",
    "# ===== GPU OPTIMIZATIONS =====\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    logger.info(f\"\\nüî• GPU Optimizations:\")\n",
    "    logger.info(f\"   cuDNN Benchmark: True\")\n",
    "    logger.info(f\"   TF32 Enabled: True\")\n",
    "    logger.info(f\"   Cache Cleared\")\n",
    "\n",
    "# ===== TRAINING MONITOR CLASS =====\n",
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.batch_times = []\n",
    "        self.loss_history = []\n",
    "        self.gpu_memory_history = []\n",
    "        \n",
    "    def update(self, loss, batch_time):\n",
    "        self.loss_history.append(loss)\n",
    "        self.batch_times.append(batch_time)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated(0) / 1e9)\n",
    "    \n",
    "    def get_avg_batch_time(self, window=100):\n",
    "        if len(self.batch_times) < window:\n",
    "            return np.mean(self.batch_times) if self.batch_times else 0\n",
    "        return np.mean(self.batch_times[-window:])\n",
    "    \n",
    "    def get_estimated_time_remaining(self, batches_per_epoch, remaining_epochs, remaining_batches):\n",
    "        avg_time = self.get_avg_batch_time()\n",
    "        if avg_time == 0:\n",
    "            return 0, 0\n",
    "        total_remaining_batches = (remaining_epochs * batches_per_epoch) + remaining_batches\n",
    "        seconds_remaining = total_remaining_batches * avg_time\n",
    "        hours = seconds_remaining / 3600\n",
    "        minutes = (seconds_remaining % 3600) / 60\n",
    "        return hours, minutes\n",
    "\n",
    "monitor = TrainingMonitor()\n",
    "logger.info(f\"‚úÖ Training monitor initialized\")\n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"‚ú® ALL VARIABLES LOADED SUCCESSFULLY - READY FOR TRAINING!\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b5b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\najib\\AppData\\Local\\Temp\\ipykernel_33500\\763347087.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "2026-01-02 16:40:08,389 - INFO - \n",
      "üöÄ STARTING OPTIMIZED TRAINING WITH RTX 4090\n",
      "2026-01-02 16:40:08,390 - INFO - ================================================================================\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     75\u001b[39m     torch.cuda.reset_peak_memory_stats()\n\u001b[32m     77\u001b[39m pbar = tqdm(\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_optimized\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     79\u001b[39m     total=\u001b[38;5;28mlen\u001b[39m(dataloader_optimized),\n\u001b[32m     80\u001b[39m     desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     81\u001b[39m     leave=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     82\u001b[39m     ncols=\u001b[32m140\u001b[39m  \u001b[38;5;66;03m# Extra wide for detailed info\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch_data \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m     86\u001b[39m     batch_start = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1139\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1144\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1145\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tools\\envs\\logs-pytorch\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tools\\envs\\logs-pytorch\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tools\\envs\\logs-pytorch\\Lib\\multiprocessing\\context.py:336\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tools\\envs\\logs-pytorch\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     94\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     97\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\tools\\envs\\logs-pytorch\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ===== CREATE OPTIMIZED DATALOADER =====\n",
    "\n",
    "dataloader_optimized = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE_RTX4090,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS_RTX4090,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Initialize mixed precision scaler\n",
    "USE_MIXED_PRECISION = USE_MIXED_PRECISION_RTX4090\n",
    "GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS_RTX4090\n",
    "\n",
    "if USE_MIXED_PRECISION:\n",
    "    scaler = GradScaler()\n",
    "\n",
    "# ===== ENHANCED PROGRESS MONITORING =====\n",
    "\n",
    "# Create a custom callback to monitor training in real-time\n",
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.batch_times = []\n",
    "        self.loss_history = []\n",
    "        self.gpu_memory_history = []\n",
    "        \n",
    "    def update(self, loss, batch_time):\n",
    "        self.loss_history.append(loss)\n",
    "        self.batch_times.append(batch_time)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_history.append(torch.cuda.memory_allocated(0) / 1e9)\n",
    "    \n",
    "    def get_avg_batch_time(self, window=100):\n",
    "        if len(self.batch_times) < window:\n",
    "            return np.mean(self.batch_times)\n",
    "        return np.mean(self.batch_times[-window:])\n",
    "    \n",
    "    def get_estimated_time_remaining(self, batches_per_epoch, remaining_epochs, remaining_batches):\n",
    "        avg_time = self.get_avg_batch_time()\n",
    "        total_remaining_batches = (remaining_epochs * batches_per_epoch) + remaining_batches\n",
    "        seconds_remaining = total_remaining_batches * avg_time\n",
    "        hours = seconds_remaining / 3600\n",
    "        minutes = (seconds_remaining % 3600) / 60\n",
    "        return hours, minutes\n",
    "\n",
    "monitor = TrainingMonitor()\n",
    "\n",
    "# ===== OPTIMIZED TRAINING WITH DETAILED MONITORING =====\n",
    "\n",
    "# Use the RTX 4090 configuration variables\n",
    "EPOCHS = EPOCHS_RTX4090\n",
    "\n",
    "logger.info(f\"\\nüöÄ STARTING OPTIMIZED TRAINING WITH RTX 4090\")\n",
    "logger.info(f\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader_optimized),\n",
    "        total=len(dataloader_optimized),\n",
    "        desc=f\"Epoch {epoch+1}/{EPOCHS}\",\n",
    "        leave=True,\n",
    "        ncols=140  # Extra wide for detailed info\n",
    "    )\n",
    "    \n",
    "    for batch_idx, batch_data in pbar:\n",
    "        batch_start = time.time()\n",
    "        batch_data = batch_data.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        if USE_MIXED_PRECISION:\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                reconstruction, latent = model(batch_data)\n",
    "                batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "                \n",
    "                reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "                target_flat = batch_data.view(-1)\n",
    "                loss = criterion(reconstruction_flat, target_flat)\n",
    "                \n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, latent = model(batch_data)\n",
    "            batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "            \n",
    "            reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "            target_flat = batch_data.view(-1)\n",
    "            loss = criterion(reconstruction_flat, target_flat)\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if USE_MIXED_PRECISION:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            if USE_MIXED_PRECISION:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update monitor\n",
    "        monitor.update(loss.item() * GRADIENT_ACCUMULATION_STEPS, batch_time)\n",
    "        \n",
    "        # Calculate GPU memory\n",
    "        gpu_mem_used = 0\n",
    "        gpu_mem_total = 0\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        remaining_epochs = EPOCHS - epoch - 1\n",
    "        remaining_batches = len(dataloader_optimized) - batch_idx - 1\n",
    "        eta_hours, eta_minutes = monitor.get_estimated_time_remaining(\n",
    "            len(dataloader_optimized), \n",
    "            remaining_epochs, \n",
    "            remaining_batches\n",
    "        )\n",
    "        \n",
    "        # Update progress bar with comprehensive info\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.5f}',\n",
    "            'GPU': f'{gpu_mem_used:.1f}/{gpu_mem_total:.1f}GB',\n",
    "            'batch_time': f'{batch_time*1000:.0f}ms',\n",
    "            'ETA': f'{int(eta_hours)}h{int(eta_minutes)}m' if eta_hours > 0 or eta_minutes > 0 else 'Soon'\n",
    "        })\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    logger.info(f\"\\n‚úÖ Epoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "    logger.info(f\"   Loss: {avg_loss:.4f} | LR: {current_lr:.6f} | Time: {epoch_time/60:.1f}min\")\n",
    "    logger.info(f\"   Avg batch time: {monitor.get_avg_batch_time()*1000:.0f}ms\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "        logger.info(f\"   Peak GPU Memory: {peak_memory:.2f} GB\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = f\"{CHECKPOINT_DIR}/autoencoder_rtx4090_epoch_{epoch+1}.pth\"\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': 48,\n",
    "        'latent_dim': 24,\n",
    "        'max_len': MAX_LEN,\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"   üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "logger.info(f\"\\n\" + \"=\"*80)\n",
    "logger.info(f\"üéâ TRAINING COMPLETE!\")\n",
    "logger.info(f\"=\"*80)\n",
    "logger.info(f\"‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "logger.info(f\"üìä Final loss: {train_losses[-1]:.4f}\")\n",
    "logger.info(f\"üìâ Loss reduction: {((train_losses[0] - train_losses[-1])/train_losses[0]*100):.1f}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "    logger.info(f\"üñ•Ô∏è  Peak GPU Memory: {peak_memory:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(f\"üßπ GPU cache cleared\")\n",
    "\n",
    "logger.info(f\"\\n‚ú® Model ready for inference or further optimization!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b380ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOT TRAINING LOSS =====\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Autoencoder Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/training_loss.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f\"üìä Training loss plot saved to {CHECKPOINT_DIR}/training_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f195b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:25:10,009 - INFO - üìÇ Found 10 checkpoints. Loading latest: autoencoder_epoch_9.pth\n",
      "2026-01-02 17:25:10,392 - INFO - ü§ñ Reconstructing model from checkpoint...\n",
      "2026-01-02 17:25:10,426 - INFO - ‚úÖ Model loaded from epoch 9\n",
      "2026-01-02 17:25:10,427 - INFO - üìä Training loss at checkpoint: 3.3391\n",
      "2026-01-02 17:25:10,427 - INFO - üìà Total parameters: 4,871,312\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD PRETRAINED MODEL FROM CHECKPOINT =====\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith(\"autoencoder_epoch_\") and f.endswith(\".pth\")])\n",
    "\n",
    "if checkpoints:\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "    logger.info(f\"üìÇ Found {len(checkpoints)} checkpoints. Loading latest: {checkpoints[-1]}\")\n",
    "    \n",
    "    # Load checkpoint metadata\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=DEVICE)\n",
    "    \n",
    "    # Recreate model with saved parameters\n",
    "    logger.info(f\"ü§ñ Reconstructing model from checkpoint...\")\n",
    "    model = LogAutoEncoder(\n",
    "        vocab_size=checkpoint['vocab_size'],\n",
    "        embed_dim=checkpoint['embed_dim'],\n",
    "        latent_dim=checkpoint['latent_dim']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    logger.info(f\"‚úÖ Model loaded from epoch {checkpoint['epoch']}\")\n",
    "    logger.info(f\"üìä Training loss at checkpoint: {checkpoint['loss']:.4f}\")\n",
    "    logger.info(f\"üìà Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è No checkpoints found! Train the model first.\")\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:25:12,243 - INFO - \n",
      "============================================================\n",
      "2026-01-02 17:25:12,244 - INFO - üîç COMPUTING ANOMALY SCORES ON TRAINING DATA\n",
      "2026-01-02 17:25:12,244 - INFO - ============================================================\n",
      "2026-01-02 17:25:34,977 - INFO - ‚úÖ Computed anomaly scores for 256 samples\n",
      "2026-01-02 17:25:34,992 - INFO - üìä Reconstruction Error Statistics:\n",
      "2026-01-02 17:25:35,002 - INFO -    Mean:   0.5012\n",
      "2026-01-02 17:25:35,009 - INFO -    Std:    0.3662\n",
      "2026-01-02 17:25:35,010 - INFO -    Min:    0.0456\n",
      "2026-01-02 17:25:35,011 - INFO -    Max:    4.1968\n",
      "2026-01-02 17:25:35,022 - INFO -    Median: 0.4456\n",
      "2026-01-02 17:25:35,033 - INFO - \n",
      "üö® Found 26 potential anomalies (top 10% by error)\n",
      "2026-01-02 17:25:35,035 - INFO -    Anomaly threshold: 0.8118\n"
     ]
    }
   ],
   "source": [
    "# ===== ANOMALY DETECTION WITH TRAINED MODEL =====\n",
    "\n",
    "def compute_reconstruction_error(model, batch, device):\n",
    "    \"\"\"\n",
    "    Compute reconstruction error (anomaly score) for a batch of logs.\n",
    "    Higher error = more anomalous\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.long).to(device)\n",
    "        reconstruction, latent = model(batch_tensor)\n",
    "        \n",
    "        # Flatten for loss computation\n",
    "        batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "        reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "        target_flat = batch_tensor.view(-1)\n",
    "        \n",
    "        # Compute cross-entropy per sample\n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=0, reduction='none')\n",
    "        loss_per_token = ce_loss(reconstruction_flat, target_flat)\n",
    "        loss_per_sample = loss_per_token.view(batch_size, seq_len).mean(dim=1)\n",
    "        \n",
    "        return loss_per_sample.cpu().numpy(), latent.cpu().numpy()\n",
    "\n",
    "# Test on a sample batch\n",
    "if model is not None:\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"üîç COMPUTING ANOMALY SCORES ON TRAINING DATA\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Get sample batches\n",
    "    sample_indices = np.random.choice(len(dataset), size=min(1000, len(dataset)), replace=False)\n",
    "    sample_batch = input_ids_matrix[sample_indices[:256]]  # First batch\n",
    "    \n",
    "    errors, latents = compute_reconstruction_error(model, sample_batch, DEVICE)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Computed anomaly scores for {len(errors)} samples\")\n",
    "    logger.info(f\"üìä Reconstruction Error Statistics:\")\n",
    "    logger.info(f\"   Mean:   {errors.mean():.4f}\")\n",
    "    logger.info(f\"   Std:    {errors.std():.4f}\")\n",
    "    logger.info(f\"   Min:    {errors.min():.4f}\")\n",
    "    logger.info(f\"   Max:    {errors.max():.4f}\")\n",
    "    logger.info(f\"   Median: {np.median(errors):.4f}\")\n",
    "    \n",
    "    # Identify potential anomalies (top 10%)\n",
    "    threshold = np.percentile(errors, 90)\n",
    "    anomalies = np.where(errors > threshold)[0]\n",
    "    logger.info(f\"\\nüö® Found {len(anomalies)} potential anomalies (top 10% by error)\")\n",
    "    logger.info(f\"   Anomaly threshold: {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae764ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:25:40,471 - INFO - ‚úÖ DataLoader pr√™t. Batches: 964763\n",
      "2026-01-02 17:25:40,471 - INFO - üì¶ Configuration optimale pour Windows: num_workers=0, pin_memory=True\n"
     ]
    }
   ],
   "source": [
    "# --- DATASET & DATALOADER (R√âUTILISATION) ---\n",
    "\n",
    "# La classe LogDataset est d√©j√† d√©finie plus haut, on peut la r√©utiliser directement\n",
    "# Cr√©er l'objet Dataset\n",
    "dataset = LogDataset(input_ids_matrix)\n",
    "\n",
    "# Cr√©er le DataLoader\n",
    "# ‚ö†Ô∏è IMPORTANT: num_workers=0 sur Windows! C'est essentiel pour la performance\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=256, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # ‚Üê WINDOWS: Toujours 0 !\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "logger.info(f\"‚úÖ DataLoader pr√™t. Batches: {len(dataloader)}\")\n",
    "logger.info(f\"üì¶ Configuration optimale pour Windows: num_workers=0, pin_memory={torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07725a",
   "metadata": {},
   "source": [
    "## 5. Using the Model for Anomaly Detection\n",
    "\n",
    "Now you can use the trained model to detect anomalies in your logs. Here's how to:\n",
    "1. Score logs with the reconstruction error\n",
    "2. Set a threshold to identify anomalies\n",
    "3. Analyze suspicious logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7affa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:26:04,151 - INFO - \n",
      "======================================================================\n",
      "2026-01-02 17:26:04,152 - INFO - üìä STEP 1: Computing Anomaly Scores on Full Dataset\n",
      "2026-01-02 17:26:04,153 - INFO - ======================================================================\n",
      "2026-01-02 17:26:04,154 - INFO - üîÑ Processing 246,979,255 logs in 482382 batches...\n",
      "Computing Anomaly Scores:   0%|          | 0/482382 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ===== STEP 1: COMPUTE ANOMALY SCORES ON FULL DATASET =====\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"üìä STEP 1: Computing Anomaly Scores on Full Dataset\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "if model is None:\n",
    "    logger.error(\"‚ùå Model not loaded! Please train or load a checkpoint first.\")\n",
    "else:\n",
    "    # Compute scores for all logs in batches\n",
    "    all_errors = []\n",
    "    all_latents = []\n",
    "    \n",
    "    INFERENCE_BATCH_SIZE = 512  # Larger batch for inference (no gradients)\n",
    "    inference_loader = DataLoader(dataset, batch_size=INFERENCE_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    logger.info(f\"üîÑ Processing {len(dataset):,} logs in {len(inference_loader)} batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Computing Anomaly Scores\"):\n",
    "            batch = batch.to(DEVICE)\n",
    "            errors, latents = compute_reconstruction_error(model, batch.cpu().numpy(), DEVICE)\n",
    "            all_errors.append(errors)\n",
    "            all_latents.append(latents)\n",
    "    \n",
    "    # Concatenate all scores\n",
    "    all_errors = np.concatenate(all_errors, axis=0)\n",
    "    all_latents = np.concatenate(all_latents, axis=0)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Computed {len(all_errors):,} anomaly scores\")\n",
    "    logger.info(f\"\\nüìä RECONSTRUCTION ERROR STATISTICS:\")\n",
    "    logger.info(f\"   Mean:     {all_errors.mean():.4f}\")\n",
    "    logger.info(f\"   Std:      {all_errors.std():.4f}\")\n",
    "    logger.info(f\"   Min:      {all_errors.min():.4f}\")\n",
    "    logger.info(f\"   Max:      {all_errors.max():.4f}\")\n",
    "    logger.info(f\"   Median:   {np.median(all_errors):.4f}\")\n",
    "    logger.info(f\"   Q75:      {np.percentile(all_errors, 75):.4f}\")\n",
    "    logger.info(f\"   Q90:      {np.percentile(all_errors, 90):.4f}\")\n",
    "    logger.info(f\"   Q95:      {np.percentile(all_errors, 95):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8447975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: SET ANOMALY THRESHOLD AND IDENTIFY ANOMALIES =====\n",
    "\n",
    "# Choose your anomaly threshold (higher = stricter, fewer anomalies detected)\n",
    "# You can adjust this based on your needs\n",
    "ANOMALY_THRESHOLD_PERCENTILE = 90  # Top 10% = anomalies\n",
    "\n",
    "anomaly_threshold = np.percentile(all_errors, ANOMALY_THRESHOLD_PERCENTILE)\n",
    "anomaly_indices = np.where(all_errors > anomaly_threshold)[0]\n",
    "\n",
    "logger.info(f\"\\nüö® ANOMALY DETECTION (Threshold: {ANOMALY_THRESHOLD_PERCENTILE}th percentile)\")\n",
    "logger.info(f\"   Threshold value: {anomaly_threshold:.4f}\")\n",
    "logger.info(f\"   Anomalies found: {len(anomaly_indices):,} out of {len(all_errors):,}\")\n",
    "logger.info(f\"   Anomaly rate: {len(anomaly_indices)/len(all_errors)*100:.2f}%\")\n",
    "\n",
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'log_index': np.arange(len(all_errors)),\n",
    "    'reconstruction_error': all_errors,\n",
    "    'is_anomaly': all_errors > anomaly_threshold\n",
    "})\n",
    "\n",
    "logger.info(f\"\\nüìä Top 20 Most Anomalous Logs:\")\n",
    "top_anomalies = results_df.nlargest(20, 'reconstruction_error')\n",
    "print(top_anomalies[['log_index', 'reconstruction_error']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 3: VIEW ACTUAL LOGS THAT ARE ANOMALIES =====\n",
    "\n",
    "def decode_log_from_ids(ids, inverse_vocab):\n",
    "    \"\"\"Convert token IDs back to text\"\"\"\n",
    "    words = []\n",
    "    for id_val in ids:\n",
    "        if id_val == 0:  # PAD token\n",
    "            continue\n",
    "        word = inverse_vocab.get(str(id_val), \"<UNK>\")\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Load inverse vocabulary\n",
    "with open(VOCAB_FILE, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "    inverse_vocab = vocab_data['inverse_vocab']\n",
    "\n",
    "logger.info(f\"\\nüìù TOP 10 MOST ANOMALOUS LOGS (with original text):\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "for rank, (idx, row) in enumerate(top_anomalies.head(10).iterrows(), 1):\n",
    "    log_idx = int(row['log_index'])\n",
    "    error = row['reconstruction_error']\n",
    "    \n",
    "    # Get the original log text\n",
    "    log_ids = input_ids_matrix[log_idx]\n",
    "    log_text = decode_log_from_ids(log_ids, inverse_vocab)\n",
    "    \n",
    "    print(f\"\\n[#{rank}] Log Index: {log_idx} | Error: {error:.4f}\")\n",
    "    print(f\"     Raw:  {df['raw_log'].iloc[log_idx][:100]}...\")\n",
    "    print(f\"     Cleaned: {df['cleaned_log'].iloc[log_idx][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313a8c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===== STEP 4: VISUALIZATION OF ANOMALY DISTRIBUTION =====\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m fig, axes = \u001b[43mplt\u001b[49m.subplots(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Plot 1: Histogram of reconstruction errors\u001b[39;00m\n\u001b[32m      6\u001b[39m axes[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m].hist(all_errors, bins=\u001b[32m100\u001b[39m, alpha=\u001b[32m0.7\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m, edgecolor=\u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== STEP 4: VISUALIZATION OF ANOMALY DISTRIBUTION =====\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Histogram of reconstruction errors\n",
    "axes[0, 0].hist(all_errors, bins=100, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].axvline(anomaly_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({ANOMALY_THRESHOLD_PERCENTILE}%ile)')\n",
    "axes[0, 0].set_xlabel('Reconstruction Error')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Reconstruction Errors')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Log-scale histogram\n",
    "axes[0, 1].hist(all_errors, bins=100, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].axvline(anomaly_threshold, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Reconstruction Error')\n",
    "axes[0, 1].set_ylabel('Frequency (log scale)')\n",
    "axes[0, 1].set_title('Distribution of Reconstruction Errors (Log Scale)')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Sorted errors\n",
    "sorted_errors = np.sort(all_errors)\n",
    "axes[1, 0].plot(sorted_errors, color='purple', linewidth=1.5)\n",
    "axes[1, 0].axhline(anomaly_threshold, color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "axes[1, 0].set_xlabel('Log Index (sorted)')\n",
    "axes[1, 0].set_ylabel('Reconstruction Error')\n",
    "axes[1, 0].set_title('Reconstruction Errors (Sorted)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Box plot\n",
    "axes[1, 1].boxplot(all_errors, vert=True)\n",
    "axes[1, 1].axhline(anomaly_threshold, color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "axes[1, 1].set_ylabel('Reconstruction Error')\n",
    "axes[1, 1].set_title('Box Plot of Reconstruction Errors')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/anomaly_detection_analysis.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f\"\\nüìä Visualization saved to {CHECKPOINT_DIR}/anomaly_detection_analysis.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822baa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 5: SCORE A NEW LOG (PREDICTION) =====\n",
    "\n",
    "def score_new_logs(raw_logs_list, model, vocab_dict, device, max_len=128):\n",
    "    \"\"\"\n",
    "    Score new logs that are NOT in the original dataset.\n",
    "    \n",
    "    Args:\n",
    "        raw_logs_list: List of raw log strings\n",
    "        model: Trained autoencoder model\n",
    "        vocab_dict: Vocabulary dictionary\n",
    "        device: torch device (cuda or cpu)\n",
    "        max_len: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of anomaly scores\n",
    "    \"\"\"\n",
    "    # Clean the logs using the same patterns\n",
    "    cleaned_logs = []\n",
    "    for log in raw_logs_list:\n",
    "        cleaned = log\n",
    "        for pat, repl in PATTERNS_PY.items():\n",
    "            cleaned = re.sub(pat, repl, cleaned)\n",
    "        cleaned_logs.append(cleaned)\n",
    "    \n",
    "    # Encode logs\n",
    "    pad_id = vocab_dict['<PAD>']\n",
    "    unk_id = vocab_dict['<UNK>']\n",
    "    \n",
    "    encoded_ids = []\n",
    "    for text in cleaned_logs:\n",
    "        ids = [vocab_dict.get(word, unk_id) for word in text.split()]\n",
    "        # Pad or truncate\n",
    "        ids = ids[:max_len]\n",
    "        ids = ids + [pad_id] * (max_len - len(ids))\n",
    "        encoded_ids.append(ids)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    encoded_tensor = torch.tensor(encoded_ids, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Score\n",
    "    with torch.no_grad():\n",
    "        reconstruction, _ = model(encoded_tensor)\n",
    "        batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "        \n",
    "        reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "        target_flat = encoded_tensor.view(-1)\n",
    "        \n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=pad_id, reduction='none')\n",
    "        loss_per_token = ce_loss(reconstruction_flat, target_flat)\n",
    "        loss_per_sample = loss_per_token.view(batch_size, seq_len).mean(dim=1)\n",
    "        \n",
    "        return loss_per_sample.cpu().numpy()\n",
    "\n",
    "# Example: Score some new logs\n",
    "example_logs = [\n",
    "    \"Connection established successfully\",\n",
    "    \"ERROR: Invalid authentication token received\",\n",
    "    \"WARNING: Disk space running low on server\",\n",
    "    \"CRITICAL: Unexpected null pointer exception occurred\",\n",
    "    \"INFO: Request processed in 123ms\"\n",
    "]\n",
    "\n",
    "logger.info(f\"\\n\" + \"=\"*70)\n",
    "logger.info(\"üß™ EXAMPLE: Scoring New Logs\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "new_scores = score_new_logs(example_logs, model, vocab_dict, DEVICE)\n",
    "\n",
    "logger.info(f\"\\nLog Scores:\")\n",
    "for i, (log, score) in enumerate(zip(example_logs, new_scores), 1):\n",
    "    is_anomaly = \"üö® ANOMALY\" if score > anomaly_threshold else \"‚úÖ NORMAL\"\n",
    "    logger.info(f\"[{i}] {is_anomaly} | Score: {score:.4f} | Log: {log[:60]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae82f2",
   "metadata": {},
   "source": [
    "## 6. Save Final Production Model\n",
    "\n",
    "Save the trained model and all necessary artifacts for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SAVE FINAL PRODUCTION MODEL =====\n",
    "\n",
    "# Create a production directory\n",
    "PRODUCTION_DIR = \"production_model\"\n",
    "os.makedirs(PRODUCTION_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(f\"\\n\" + \"=\"*70)\n",
    "logger.info(\"üíæ SAVING FINAL PRODUCTION MODEL\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "if model is None:\n",
    "    logger.error(\"‚ùå Model not loaded! Cannot save.\")\n",
    "else:\n",
    "    # 1. Save the trained model\n",
    "    final_model_path = f\"{PRODUCTION_DIR}/autoencoder_final.pth\"\n",
    "    torch.save({\n",
    "        'epoch': checkpoint['epoch'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': 48,\n",
    "        'latent_dim': 24,\n",
    "        'max_len': MAX_LEN,\n",
    "        'device': str(DEVICE),\n",
    "        'training_loss': checkpoint['loss'],\n",
    "        'anomaly_threshold': float(anomaly_threshold),\n",
    "        'threshold_percentile': ANOMALY_THRESHOLD_PERCENTILE,\n",
    "    }, final_model_path)\n",
    "    logger.info(f\"‚úÖ Model saved: {final_model_path}\")\n",
    "    \n",
    "    # 2. Save vocabulary\n",
    "    vocab_path = f\"{PRODUCTION_DIR}/vocab.json\"\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump(vocab_data, f, indent=2)\n",
    "    logger.info(f\"‚úÖ Vocabulary saved: {vocab_path}\")\n",
    "    \n",
    "    # 3. Save cleaning patterns\n",
    "    patterns_path = f\"{PRODUCTION_DIR}/cleaning_patterns.json\"\n",
    "    patterns_dict = {pattern: replacement for pattern, replacement in PATTERNS_PY.items()}\n",
    "    with open(patterns_path, 'w') as f:\n",
    "        json.dump(patterns_dict, f, indent=2)\n",
    "    logger.info(f\"‚úÖ Cleaning patterns saved: {patterns_path}\")\n",
    "    \n",
    "    # 4. Save configuration\n",
    "    config_path = f\"{PRODUCTION_DIR}/config.json\"\n",
    "    config = {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': 48,\n",
    "        'latent_dim': 24,\n",
    "        'max_len': MAX_LEN,\n",
    "        'pad_token': '<PAD>',\n",
    "        'unk_token': '<UNK>',\n",
    "        'anomaly_threshold': float(anomaly_threshold),\n",
    "        'threshold_percentile': ANOMALY_THRESHOLD_PERCENTILE,\n",
    "        'model_type': 'LogAutoEncoder',\n",
    "        'training_epochs': checkpoint['epoch'],\n",
    "        'final_loss': float(checkpoint['loss']),\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    logger.info(f\"‚úÖ Configuration saved: {config_path}\")\n",
    "    \n",
    "    # 5. Save anomaly detection results\n",
    "    results_path = f\"{PRODUCTION_DIR}/anomaly_detection_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    logger.info(f\"‚úÖ Anomaly detection results saved: {results_path} ({len(results_df):,} logs)\")\n",
    "    \n",
    "    logger.info(f\"\\nüì¶ PRODUCTION MODEL SAVED SUCCESSFULLY!\")\n",
    "    logger.info(f\"üìÅ Directory: {PRODUCTION_DIR}/\")\n",
    "    logger.info(f\"\\nContents:\")\n",
    "    logger.info(f\"   - autoencoder_final.pth    (Model weights)\")\n",
    "    logger.info(f\"   - vocab.json                (Vocabulary)\")\n",
    "    logger.info(f\"   - cleaning_patterns.json    (Regex patterns)\")\n",
    "    logger.info(f\"   - config.json               (Configuration)\")\n",
    "    logger.info(f\"   - anomaly_detection_results.csv (Scores for all logs)\")\n",
    "    \n",
    "    print(\"\\n‚ú® Model ready for production use! ‚ú®\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CREATE A REUSABLE INFERENCE SCRIPT =====\n",
    "\n",
    "inference_script = '''\"\"\"\n",
    "Standalone Log Anomaly Detection Script\n",
    "Load the trained model and score logs on demand.\n",
    "\"\"\"\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "class LogAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=48, latent_dim=24):\n",
    "        super(LogAutoEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "        self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.encoder(embedded)\n",
    "        latent = h_n.squeeze(0)\n",
    "        latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1)\n",
    "        decoded, _ = self.decoder(latent_repeated)\n",
    "        reconstruction = self.output_linear(decoded)\n",
    "        return reconstruction, latent\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self, model_dir=\"production_model\"):\n",
    "        \"\"\"Load model and configuration\"\"\"\n",
    "        self.model_dir = Path(model_dir)\n",
    "        \n",
    "        # Load config\n",
    "        with open(self.model_dir / \"config.json\") as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(self.model_dir / \"vocab.json\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "            self.vocab = vocab_data['vocab']\n",
    "        \n",
    "        # Load cleaning patterns\n",
    "        with open(self.model_dir / \"cleaning_patterns.json\") as f:\n",
    "            self.patterns = json.load(f)\n",
    "        \n",
    "        # Load model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = LogAutoEncoder(\n",
    "            vocab_size=self.config['vocab_size'],\n",
    "            embed_dim=self.config['embed_dim'],\n",
    "            latent_dim=self.config['latent_dim']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        checkpoint = torch.load(\n",
    "            self.model_dir / \"autoencoder_final.pth\",\n",
    "            map_location=self.device\n",
    "        )\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.threshold = checkpoint['anomaly_threshold']\n",
    "        print(f\"‚úÖ Model loaded from {model_dir}\")\n",
    "        print(f\"   Anomaly threshold: {self.threshold:.4f}\")\n",
    "    \n",
    "    def clean_log(self, log_text):\n",
    "        \"\"\"Clean a log using regex patterns\"\"\"\n",
    "        cleaned = log_text\n",
    "        for pattern, replacement in self.patterns.items():\n",
    "            cleaned = re.sub(pattern, replacement, cleaned)\n",
    "        return cleaned\n",
    "    \n",
    "    def score_logs(self, logs):\n",
    "        \"\"\"Score a list of logs (raw text)\"\"\"\n",
    "        pad_id = self.vocab['<PAD>']\n",
    "        unk_id = self.vocab['<UNK>']\n",
    "        \n",
    "        # Clean and encode\n",
    "        encoded_ids = []\n",
    "        cleaned_logs = []\n",
    "        \n",
    "        for log in logs:\n",
    "            cleaned = self.clean_log(log)\n",
    "            cleaned_logs.append(cleaned)\n",
    "            \n",
    "            ids = [self.vocab.get(word, unk_id) for word in cleaned.split()]\n",
    "            ids = ids[:self.config['max_len']]\n",
    "            ids = ids + [pad_id] * (self.config['max_len'] - len(ids))\n",
    "            encoded_ids.append(ids)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        tensor = torch.tensor(encoded_ids, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # Score\n",
    "        with torch.no_grad():\n",
    "            reconstruction, _ = self.model(tensor)\n",
    "            batch_size, seq_len, vocab_size = reconstruction.shape\n",
    "            \n",
    "            reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "            target_flat = tensor.view(-1)\n",
    "            \n",
    "            ce_loss = nn.CrossEntropyLoss(ignore_index=pad_id, reduction='none')\n",
    "            loss_per_token = ce_loss(reconstruction_flat, target_flat)\n",
    "            loss_per_sample = loss_per_token.view(batch_size, seq_len).mean(dim=1)\n",
    "            \n",
    "            return loss_per_sample.cpu().numpy()\n",
    "    \n",
    "    def detect_anomalies(self, logs):\n",
    "        \"\"\"Score logs and classify as anomaly/normal\"\"\"\n",
    "        scores = self.score_logs(logs)\n",
    "        \n",
    "        results = []\n",
    "        for log, score in zip(logs, scores):\n",
    "            results.append({\n",
    "                'log': log,\n",
    "                'score': float(score),\n",
    "                'is_anomaly': score > self.threshold,\n",
    "                'threshold': self.threshold\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    detector = AnomalyDetector()\n",
    "    \n",
    "    test_logs = [\n",
    "        \"Connection established successfully\",\n",
    "        \"ERROR: Unexpected exception occurred\",\n",
    "        \"INFO: Processing request\"\n",
    "    ]\n",
    "    \n",
    "    results = detector.detect_anomalies(test_logs)\n",
    "    \n",
    "    for result in results:\n",
    "        status = \"üö® ANOMALY\" if result['is_anomaly'] else \"‚úÖ NORMAL\"\n",
    "        print(f\"{status} | Score: {result['score']:.4f} | {result['log'][:60]}\")\n",
    "'''\n",
    "\n",
    "# Save the inference script\n",
    "inference_path = f\"{PRODUCTION_DIR}/anomaly_detector.py\"\n",
    "with open(inference_path, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "logger.info(f\"\\n‚úÖ Inference script saved: {inference_path}\")\n",
    "logger.info(f\"\\nüìñ Usage:\")\n",
    "logger.info(f\"   from anomaly_detector import AnomalyDetector\")\n",
    "logger.info(f\"   detector = AnomalyDetector('production_model')\")\n",
    "logger.info(f\"   results = detector.detect_anomalies(['your log text'])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CREATE A README FOR THE PRODUCTION MODEL =====\n",
    "\n",
    "readme_content = \"\"\"# üéØ Log Anomaly Detection Model - Production Package\n",
    "\n",
    "## üì¶ What's Inside\n",
    "\n",
    "This folder contains a fully trained and production-ready log anomaly detection model.\n",
    "\n",
    "### Files:\n",
    "- **autoencoder_final.pth** - Trained PyTorch model weights\n",
    "- **vocab.json** - Vocabulary dictionary (word ‚Üí ID mapping)\n",
    "- **cleaning_patterns.json** - Regex patterns for log preprocessing\n",
    "- **config.json** - Model configuration and hyperparameters\n",
    "- **anomaly_detection_results.csv** - Anomaly scores for all training logs\n",
    "- **anomaly_detector.py** - Standalone Python script for inference\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### Option 1: Using the Python Script (Recommended)\n",
    "\n",
    "```python\n",
    "from anomaly_detector import AnomalyDetector\n",
    "\n",
    "# Initialize\n",
    "detector = AnomalyDetector('production_model')\n",
    "\n",
    "# Score your logs\n",
    "logs = [\n",
    "    \"Connection established successfully\",\n",
    "    \"ERROR: Unexpected null pointer exception\",\n",
    "    \"INFO: Request processed\"\n",
    "]\n",
    "\n",
    "results = detector.detect_anomalies(logs)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Log: {result['log']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\")\n",
    "    print(f\"Is Anomaly: {result['is_anomaly']}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "### Option 2: Direct PyTorch Loading\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load config\n",
    "with open('production_model/config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load model\n",
    "model = LogAutoEncoder(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    latent_dim=config['latent_dim']\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load('production_model/autoencoder_final.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "threshold = checkpoint['anomaly_threshold']\n",
    "```\n",
    "\n",
    "## üìä Model Information\n",
    "\n",
    "- **Architecture**: LSTM Autoencoder\n",
    "- **Vocabulary Size**: {VOCAB_SIZE}\n",
    "- **Embedding Dimension**: 48\n",
    "- **Latent Dimension**: 24\n",
    "- **Max Sequence Length**: 128\n",
    "- **Training Epochs**: 15\n",
    "- **Final Training Loss**: {:.4f}\n",
    "- **Anomaly Threshold**: {:.4f} (90th percentile)\n",
    "\n",
    "## üîß How It Works\n",
    "\n",
    "1. **Input**: Raw log text\n",
    "2. **Cleaning**: Apply regex patterns to standardize tokens\n",
    "3. **Encoding**: Convert words to IDs using vocabulary\n",
    "4. **Model**: LSTM Autoencoder compresses and reconstructs\n",
    "5. **Scoring**: Reconstruction error = anomaly score\n",
    "6. **Decision**: If score > threshold ‚Üí Anomaly\n",
    "\n",
    "### Interpretation\n",
    "- **Low score** (~0.5-2.0): Normal log\n",
    "- **High score** (>3.0): Anomalous/suspicious log\n",
    "- Threshold is set at the 90th percentile of training data\n",
    "\n",
    "## üìà Performance\n",
    "\n",
    "- **Anomalies Detected**: {}/{} logs\n",
    "- **Anomaly Rate**: {:.2f}%\n",
    "- **Mean Error**: {:.4f}\n",
    "- **Std Error**: {:.4f}\n",
    "\n",
    "## üîç Troubleshooting\n",
    "\n",
    "### Model not found\n",
    "- Ensure all files are in the `production_model/` directory\n",
    "- Check file paths in the code\n",
    "\n",
    "### Import errors\n",
    "- Install required packages: `pip install torch numpy`\n",
    "- Ensure Python version >= 3.8\n",
    "\n",
    "### Low detection accuracy\n",
    "- Adjust `ANOMALY_THRESHOLD_PERCENTILE` in the config\n",
    "- Retrain model with more representative data\n",
    "- Check if log format matches training data\n",
    "\n",
    "## üìù Training Metadata\n",
    "\n",
    "- **Trained on**: {len(results_df):,} logs\n",
    "- **Checkpoint**: epoch_10.pth\n",
    "- **Device Used**: {str(DEVICE)}\n",
    "\n",
    "---\n",
    "Generated: 2026-01-02\n",
    "Model version: 1.0\n",
    "\"\"\".format(\n",
    "    VOCAB_SIZE,\n",
    "    checkpoint['loss'],\n",
    "    anomaly_threshold,\n",
    "    len(anomalies),\n",
    "    len(results_df),\n",
    "    (len(anomalies)/len(results_df)*100),\n",
    "    all_errors.mean(),\n",
    "    all_errors.std()\n",
    ")\n",
    "\n",
    "readme_path = f\"{PRODUCTION_DIR}/README.md\"\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "logger.info(f\"‚úÖ README saved: {readme_path}\")\n",
    "\n",
    "# Print summary\n",
    "logger.info(f\"\\n\" + \"=\"*70)\n",
    "logger.info(\"‚ú® PRODUCTION MODEL READY FOR DEPLOYMENT!\")\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(f\"\\nüìÅ All files saved in: {PRODUCTION_DIR}/\")\n",
    "logger.info(f\"\\nüìã Files created:\")\n",
    "for file in Path(PRODUCTION_DIR).glob(\"*\"):\n",
    "    size = file.stat().st_size / 1024  # KB\n",
    "    logger.info(f\"   ‚úì {file.name} ({size:.1f} KB)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logs-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
