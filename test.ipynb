{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50e997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================\n",
    "\n",
    "# --- PATHS ---\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "MODEL_FILE = \"model.pth\" # Or \"autoencoder_epoch_X.pth\"\n",
    "VOCAB_FILE = os.path.join(CHECKPOINT_DIR, \"vocab_full.json\")\n",
    "\n",
    "# --- TEST FILE (CHANGE THIS!) ---\n",
    "# Point this to the text file you want to test (ex: a new log file)\n",
    "TEST_FILE_PATH = r\"data/kafka/log-2018-06-01/129546168_2018-05-31-23-37-13.txt\"\n",
    "# If you want to test the training set, put a path here instead:\n",
    "# TEST_FILE_PATH = \"data/kafka/log-2018-06-01/129546169_2018-05-31-23-46-03.txt\"\n",
    "\n",
    "# --- CONFIG ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Regex Patterns (Same as training)\n",
    "PATTERNS_PY = {\n",
    "    r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}Z\": \"[TIMESTAMP_ISO]\",\n",
    "    r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\": \"[TIMESTAMP]\",\n",
    "    r\"PID\\s+\\d+\": \"PID [PID_NUM]\",\n",
    "    r\"\\[\\d+\\]\": \"[ID]\",\n",
    "    r\"(?:\\d{1,3}\\.){3}\\d{1,3}\": \"[IP_ADDR]\",\n",
    "    r\"\\s+\": \" \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fd4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 2. MODEL DEFINITION (Must be identical to training)\n",
    "# ==========================\n",
    "\n",
    "class LogAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=48, latent_dim=24, max_len=128):\n",
    "        super(LogAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "        self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.encoder(embedded)\n",
    "        latent = h_n.squeeze(0)\n",
    "        latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1)\n",
    "        # Decoder expects hidden/state of size (num_layers, batch, embed_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.embed_dim, device=x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.embed_dim, device=x.device)\n",
    "        decoded, _ = self.decoder(latent_repeated, (h0, c0))\n",
    "        reconstruction = self.output_linear(decoded)\n",
    "        return reconstruction, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0099d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3. HELPER FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Apply regex patterns to clean a single log.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    for pat, repl in PATTERNS_PY.items():\n",
    "        text = re.sub(pat, repl, text)\n",
    "    return text\n",
    "\n",
    "def text_to_ids(text, vocab, unk_id, pad_id, max_len):\n",
    "    \"\"\"Convert text string to padded IDs.\"\"\"\n",
    "    if not isinstance(text, str): return [pad_id] * max_len\n",
    "    tokens = text.split()\n",
    "    ids = [vocab.get(word, unk_id) for word in tokens]\n",
    "    \n",
    "    # Truncate or Pad\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    else:\n",
    "        ids = ids + [pad_id] * (max_len - len(ids))\n",
    "    \n",
    "    return ids\n",
    "\n",
    "def calculate_reconstruction_loss(model, input_ids, vocab_size, criterion, device):\n",
    "    \"\"\"Pass data through model and calculate loss (Anomaly Score).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = model(input_tensor)\n",
    "        \n",
    "        # Calculate loss\n",
    "        input_flat = input_tensor.view(-1)\n",
    "        reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "        \n",
    "        loss = criterion(reconstruction_flat, input_flat)\n",
    "        \n",
    "    return loss.item(), latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb218ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3.5 DRAIN ALGORITHM (Simplified)\n",
    "# ==========================\n",
    "\n",
    "class SimpleDrain:\n",
    "    \"\"\"\n",
    "    Simplified Drain algorithm for log template extraction.\n",
    "    Groups logs by length, then finds similar patterns to extract templates.\n",
    "    \"\"\"\n",
    "    def __init__(self, similarity_threshold=0.5, max_depth=4):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.max_depth = max_depth\n",
    "        self.templates = {}  # template_id -> template_tokens\n",
    "        self.template_counter = 0\n",
    "        \n",
    "    def tokenize(self, log_text):\n",
    "        \"\"\"Split log into tokens.\"\"\"\n",
    "        return log_text.split()\n",
    "    \n",
    "    def get_similarity(self, tokens1, tokens2):\n",
    "        \"\"\"Calculate similarity between two token sequences.\"\"\"\n",
    "        if len(tokens1) != len(tokens2):\n",
    "            return 0.0\n",
    "        matches = sum(1 for t1, t2 in zip(tokens1, tokens2) if t1 == t2)\n",
    "        return matches / len(tokens1) if len(tokens1) > 0 else 0.0\n",
    "    \n",
    "    def find_best_template(self, tokens):\n",
    "        \"\"\"Find the best matching template for given tokens.\"\"\"\n",
    "        best_template_id = None\n",
    "        best_similarity = 0.0\n",
    "        \n",
    "        # Filter templates by same length\n",
    "        token_len = len(tokens)\n",
    "        for template_id, template_tokens in self.templates.items():\n",
    "            if len(template_tokens) == token_len:\n",
    "                similarity = self.get_similarity(tokens, template_tokens)\n",
    "                if similarity >= self.similarity_threshold and similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_template_id = template_id\n",
    "        \n",
    "        return best_template_id, best_similarity\n",
    "    \n",
    "    def create_template(self, tokens1, tokens2):\n",
    "        \"\"\"Merge two token sequences into a template with wildcards.\"\"\"\n",
    "        template = []\n",
    "        for t1, t2 in zip(tokens1, tokens2):\n",
    "            if t1 == t2:\n",
    "                template.append(t1)\n",
    "            else:\n",
    "                template.append(\"<*>\")  # Wildcard for variable parts\n",
    "        return template\n",
    "    \n",
    "    def extract_parameters(self, tokens, template_tokens):\n",
    "        \"\"\"Extract variable parameters from tokens using template.\"\"\"\n",
    "        parameters = []\n",
    "        for token, template_token in zip(tokens, template_tokens):\n",
    "            if template_token == \"<*>\":\n",
    "                parameters.append(token)\n",
    "        return parameters\n",
    "    \n",
    "    def parse(self, log_text):\n",
    "        \"\"\"\n",
    "        Parse a log line and return (template_id, template_str, parameters).\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(log_text)\n",
    "        \n",
    "        if not tokens:\n",
    "            return None, \"\", []\n",
    "        \n",
    "        # Find best matching template\n",
    "        template_id, similarity = self.find_best_template(tokens)\n",
    "        \n",
    "        if template_id is not None:\n",
    "            # Use existing template\n",
    "            template_tokens = self.templates[template_id]\n",
    "            parameters = self.extract_parameters(tokens, template_tokens)\n",
    "            template_str = \" \".join(template_tokens)\n",
    "            return template_id, template_str, parameters\n",
    "        else:\n",
    "            # Create new template\n",
    "            self.template_counter += 1\n",
    "            template_id = self.template_counter\n",
    "            \n",
    "            # First occurrence: template = exact tokens\n",
    "            self.templates[template_id] = tokens.copy()\n",
    "            template_str = \" \".join(tokens)\n",
    "            parameters = []  # No wildcards yet\n",
    "            \n",
    "            return template_id, template_str, parameters\n",
    "    \n",
    "    def update_template(self, template_id, new_tokens):\n",
    "        \"\"\"Update an existing template by merging with new tokens.\"\"\"\n",
    "        if template_id in self.templates:\n",
    "            old_template = self.templates[template_id]\n",
    "            merged_template = self.create_template(old_template, new_tokens)\n",
    "            self.templates[template_id] = merged_template\n",
    "\n",
    "# Initialize Drain parser\n",
    "drain_parser = SimpleDrain(similarity_threshold=0.5, max_depth=4)\n",
    "\n",
    "print(\"âœ… Drain parser initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7332745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Loading Model: model.pth\n",
      "ðŸ“š Loading Vocab: checkpoints\\vocab_full.json\n",
      "âœ… Vocab loaded. Size: 50000\n",
      "âœ… Model weights loaded (direct).\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 4. LOADING MODEL & VOCAB\n",
    "# ==========================\n",
    "\n",
    "print(f\"ðŸ¤– Loading Model: {MODEL_FILE}\")\n",
    "print(f\"ðŸ“š Loading Vocab: {VOCAB_FILE}\")\n",
    "\n",
    "# Load Vocab\n",
    "with open(VOCAB_FILE, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab = vocab_data['vocab']\n",
    "vocab_size = vocab_data['size']\n",
    "unk_id = vocab[vocab_data['unk_token']]\n",
    "pad_id = vocab[vocab_data['pad_token']]\n",
    "\n",
    "print(f\"âœ… Vocab loaded. Size: {vocab_size}\")\n",
    "\n",
    "# Load Model\n",
    "model = LogAutoEncoder(vocab_size=vocab_size).to(DEVICE)\n",
    "\n",
    "# Load Checkpoint\n",
    "try:\n",
    "    checkpoint = torch.load(MODEL_FILE, map_location=DEVICE)\n",
    "    \n",
    "    # Check if checkpoint has 'model_state_dict' or is just weights\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"âœ… Model weights loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(f\"âœ… Model weights loaded (direct).\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Model file not found: {MODEL_FILE}\")\n",
    "    exit()\n",
    "\n",
    "# Setup Loss function (Must match training: ignore padding)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-log test result:\n",
      "Raw (first 120 chars): 16:37:15     INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test\n",
      "16:37:15     INFO - Run as scripts...\n",
      "Cleaned (first 120 chars): 16:37:15 INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test 16:37:15 INFO - Run as scripts/scripts...\n",
      "Anomaly score: 11.4067\n"
     ]
    }
   ],
   "source": [
    "# Quick single-log test (no file read) - WITH DRAIN\n",
    "SAMPLE_LOG = \"\"\"16:37:15     INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test\\n16:37:15     INFO - Run as scripts/scripts/desktop_unittest.py --cfg unittests/mac_unittest.py --mochitest-suite browser-chrome-chunked --e10s --total-chunks 7 --this-chunk 5 --blob-upload-branch mozilla-esr52 --download-symbols true\\n16:37:15     INFO - Dumping config to /builds/slave/test/logs/localconfig.json.\\n16:37:15     INFO - {'all_cppunittest_suites': {'cppunittest': ('tests/cppunittest',)},\\n16:37:15     INFO -  'all_gtest_suites': {'gtest': ()},\\n16:37:15     INFO -  'all_jittest_suites': {'jittest': ()},\\n16:37:15     INFO -  'all_mochitest_suites': {'a11y': ('--flavor=a11y',),\\n16:37:15     INFO -                           'browser-chrome': ('--flavor=browser',),\\n16:37:15     INFO -                           'browser-chrome-addons': ('--flavor=browser',\\n16:37:15     INFO -                                                     '--chunk-by-runtime',\\n16:37:15     INFO -                                                     '--tag=addons'),\\n16:37:15     INFO -                           'browser-chrome-chunked': ('--flavor=browser',\\n16:37:15     INFO -                                                      '--chunk-by-runtime'),\\n16:37:15     INFO -                           'browser-chrome-clipboard': ('--flavor=browser',\\n16:37:15     INFO -                                                        '--subsuite=clipboard'),\\n16:37:15     INFO -                           'browser-chrome-gpu': ('--flavor=browser',\\n16:37:15     INFO -                                                  '--subsuite=gpu'),\\n16:37:15     INFO -                           'browser-chrome-screenshots': ('--flavor=browser',\\n16:37:15     INFO -                                                          '--subsuite=screenshots'),\\n16:37:15     INFO -                           'chrome': ('--flavor=chrome',),\\n16:37:15     INFO -                           'chrome-chunked': ('--flavor=chrome',\\n16:37:15     INFO -                                              '--chunk-by-dir=4'),\\n16:37:15     INFO -                           'chrome-clipboard': ('--flavor=chrome',\\n16:37:15     INFO -                                                '--subsuite=clipboard'),\\n16:37:15     INFO -                           'chrome-gpu': ('--flavor=chrome', '--subsuite=gpu'),\\n16:37:15     INFO -                           'jetpack-addon': ('--flavor=jetpack-addon',),\\n16:37:15     INFO -                           'jetpack-package': ('--flavor=jetpack-package',),\\n16:37:15     INFO -                           'jetpack-package-clipboard': ('--flavor=jetpack-package',\\n16:37:15     INFO -                                                         '--subsuite=clipboard'),\\n16:37:15     INFO -                           'mochitest-devtools-chrome': ('--flavor=browser',\\n16:37:15     INFO -                                                         '--subsuite=devtools'),\\n16:37:15     INFO -                           'mochitest-devtools-chrome-chunked': ('--flavor=browser',\\n16:37:15     INFO -                                                                 '--subsuite=devtools',\\n16:37:15     INFO -                                                                 '--chunk-by-runtime'),\\n16:37:15     INFO -                           'mochitest-gl': ('--subsuite=webgl',),\\n16:37:15     INFO -                           'mochitest-media': ('--subsuite=media',),\\n16:37:15     INFO -                           'plain': (),\\n16:37:15     INFO -                           'plain-chunked': ('--chunk-by-dir=4',),\\n16:37:15     INFO -                           'plain-clipboard': ('--subsuite=clipboard',),\\n16:37:15     INFO -                           'plain-gpu': ('--subsuite=gpu',)},\\n16:37:15     INFO -  'all_mozbase_suites': {'mozbase': ()},\\n16:37:15     INFO -  'all_reftest_suites': {'crashtest': {'options': ('--suite=crashtest',),\\n16:37:15     INFO -                                       'tests': ('tests/reftest/tests/testing/crashtest/crashtests.list',)},\\n16:37:15     INFO -                         'jsreftest': {'options': ('--extra-profile-file=tests/jsreftest/tests/user.js',),\\n16:37:15     INFO -                                       'tests': ('tests/jsreftest/tests/jstests.list',)},\\n16:37:15     INFO -                         'reftest': {'options': ('--suite=reftest',),\\n16:37:15     INFO -                                     'tests': ('tests/reftest/tests/layout/reftests/reftest.list',)}},\\n16:37:15     INFO -  'all_xpcshell_suites': {'xpcshell': {'options': ('--xpcshell=%(abs_app_dir)s/xpcshell',\\n16:37:15     INFO -                                                   '--manifest=tests/xpcshell/tests/xpcshell.ini'),\\n16:37:15     INFO -                                       'tests': ()},\\n16:37:15     INFO -                          'xpcshell-addons': {'options': ('--xpcshell=%(abs_app_dir)s/xpcshell',\\n16:37:15     INFO -                                                          '--tag=addons',\\n16:37:15     INFO -                                                          '--manifest=tests/xpcshell/tests/xpcshell.ini'),\\n16:37:15     INFO -                                              'tests': ()}},\\n16:37:15     INFO -  'allow_software_gl_layers': False,\\n16:37:15     INFO -  'append_to_log': False,\\n16:37:15     INFO -  'base_work_dir': '/builds/slave/test',\\n16:37:15     INFO -  'blob_upload_branch': 'mozilla-esr52',\\n16:37:15     INFO -  'blob_uploader_auth_file': '/builds/slave/test/oauth.txt',\\n16:37:15     INFO -  'buildbot_json_path': 'buildprops.json',\\n16:37:15     INFO -  'buildbot_max_log_size': 52428800,\\n16:37:15     INFO -  'code_coverage': False\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Step 1: Clean\n",
    "    cleaned = clean_text(SAMPLE_LOG)\n",
    "    \n",
    "    # Step 2: DRAIN - Extract template and parameters\n",
    "    template_id, template_str, parameters = drain_parser.parse(cleaned)\n",
    "    \n",
    "    # Step 3: Encode (use template instead of full cleaned log)\n",
    "    ids = text_to_ids(template_str, vocab, unk_id, pad_id, MAX_LEN)\n",
    "    \n",
    "    # Step 4: Calculate anomaly score\n",
    "    loss, latent = calculate_reconstruction_loss(model, ids, vocab_size, criterion, DEVICE)\n",
    "\n",
    "print(\"Single-log test result (with Drain):\")\n",
    "print(f\"Raw (first 120 chars): {SAMPLE_LOG[:120]}...\")\n",
    "print(f\"Cleaned (first 120 chars): {cleaned[:120]}...\")\n",
    "print(f\"Template ID: {template_id}\")\n",
    "print(f\"Template (first 120 chars): {template_str[:120]}...\")\n",
    "print(f\"Parameters extracted: {parameters[:5]}...\" if len(parameters) > 5 else f\"Parameters: {parameters}\")\n",
    "print(f\"Anomaly score: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c61fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Reading file: data/kafka/log-2018-06-01/129546168_2018-05-31-23-37-13.txt\n",
      "ðŸ”„ Processing & Calculating Anomaly Scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd0f64e80634166978d949c1020ab24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Logs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================\n",
    "# 5. RUNNING THE TEST - WITH DRAIN\n",
    "# ==========================\n",
    "\n",
    "# Ensure file exists\n",
    "if not os.path.exists(TEST_FILE_PATH):\n",
    "    print(f\"âŒ Test file not found: {TEST_FILE_PATH}\")\n",
    "    # Fallback: Try listing directory\n",
    "    print(f\"   Looking in: {os.path.dirname(TEST_FILE_PATH)}\")\n",
    "    if os.path.exists(os.path.dirname(TEST_FILE_PATH)):\n",
    "        files = os.listdir(os.path.dirname(TEST_FILE_PATH))\n",
    "        print(f\"   Files found: {files[:5]}\")\n",
    "    exit()\n",
    "\n",
    "# Process file\n",
    "results = []\n",
    "\n",
    "print(f\"ðŸ“‚ Reading file: {TEST_FILE_PATH}\")\n",
    "print(f\"ðŸ”„ Processing with Drain & Calculating Anomaly Scores...\")\n",
    "\n",
    "# We read line by line to avoid OOM\n",
    "try:\n",
    "    with open(TEST_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in tqdm(f, desc=\"Testing Logs\"):\n",
    "            if not line.strip(): continue\n",
    "            \n",
    "            # 1. Clean\n",
    "            cleaned = clean_text(line)\n",
    "            \n",
    "            # 2. DRAIN - Extract template and parameters\n",
    "            template_id, template_str, parameters = drain_parser.parse(cleaned)\n",
    "            \n",
    "            # 3. Encode (use template for anomaly detection)\n",
    "            ids = text_to_ids(template_str, vocab, unk_id, pad_id, MAX_LEN)\n",
    "            \n",
    "            # 4. Predict\n",
    "            loss, latent = calculate_reconstruction_loss(model, ids, vocab_size, criterion, DEVICE)\n",
    "            \n",
    "            results.append({\n",
    "                'raw_log': line,\n",
    "                'cleaned_log': cleaned,\n",
    "                'template_id': template_id,\n",
    "                'template': template_str,\n",
    "                'parameters': str(parameters),  # Convert list to string for CSV\n",
    "                'num_parameters': len(parameters),\n",
    "                'anomaly_score': loss,\n",
    "                'latent_vector': latent.cpu().numpy().tolist()\n",
    "            })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error processing file: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(results)} log lines\")\n",
    "print(f\"ðŸ” Discovered {len(drain_parser.templates)} unique templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 5.5 DRAIN TEMPLATES ANALYSIS\n",
    "# ==========================\n",
    "\n",
    "print(f\"\\nðŸ“‹ DRAIN TEMPLATE ANALYSIS\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Total unique templates discovered: {len(drain_parser.templates)}\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "# Show top 10 most common templates\n",
    "from collections import Counter\n",
    "template_counts = Counter([r['template_id'] for r in results])\n",
    "\n",
    "print(f\"\\nTop 10 Most Frequent Templates:\")\n",
    "print(\"-\" * 70)\n",
    "for template_id, count in template_counts.most_common(10):\n",
    "    template = drain_parser.templates.get(template_id, [])\n",
    "    template_str = \" \".join(template)\n",
    "    # Truncate long templates\n",
    "    display_template = template_str[:80] + \"...\" if len(template_str) > 80 else template_str\n",
    "    print(f\"Template {template_id:3d} | Count: {count:5d} | {display_template}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š ANOMALY SCORES SUMMARY\n",
      "==================================================\n",
      "Mean Score:    3.2490\n",
      "Std Deviation:   1.4185\n",
      "Max Score:      14.0555\n",
      "Threshold (M+3*SD): 7.5045\n",
      "==================================================\n",
      "\n",
      "ðŸš¨ Found 318 Anomalies (Score > 7.5045)\n",
      "==================================================\n",
      "Top 5 Most Anomalous Logs:\n",
      "--------------------------------------------------\n",
      "Score: 14.0555 | Log: Saving to: 'archiver_client.py'\n",
      "\n",
      "Score: 11.8500 | Log: Connecting to hg.mozilla.org|63.245.215.102|:443... connected.\n",
      "\n",
      "Score: 11.6276 | Log: rm -f oauth.txt\n",
      "\n",
      "Score: 11.6031 | Log: rm -rf scripts properties\n",
      "\n",
      "Score: 11.3854 | Log:   PROPERTIES_FILE=/builds/slave/test/buildprops.json\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ’¾ Results saved to: checkpoints/test_anomaly_results.csv\n",
      "ðŸ’¾ Anomalies saved to: checkpoints/anomalies_only.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 6. ANALYZE & VISUALIZATION\n",
    "# ==========================\n",
    "\n",
    "# Convert to Pandas for analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Calculate Statistics\n",
    "mean_score = df_results['anomaly_score'].mean()\n",
    "std_score = df_results['anomaly_score'].std()\n",
    "max_score = df_results['anomaly_score'].max()\n",
    "\n",
    "# Define Threshold (Mean + 3*Std is a common heuristic)\n",
    "anomaly_threshold = mean_score + 3 * std_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ“Š ANOMALY SCORES SUMMARY (Template-based)\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Mean Score:          {mean_score:.4f}\")\n",
    "print(f\"Std Deviation:       {std_score:.4f}\")\n",
    "print(f\"Max Score:           {max_score:.4f}\")\n",
    "print(f\"Threshold (M+3*SD):  {anomaly_threshold:.4f}\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "# Highlight Anomalies (Logs above threshold)\n",
    "anomalies = df_results[df_results['anomaly_score'] > anomaly_threshold]\n",
    "\n",
    "print(f\"\\nðŸš¨ Found {len(anomalies)} Anomalies (Score > {anomaly_threshold:.4f})\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "# Show top 5 most anomalous logs with their templates\n",
    "print(\"Top 5 Most Anomalous Logs:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in anomalies.nlargest(5, 'anomaly_score').iterrows():\n",
    "    print(f\"Score: {row['anomaly_score']:.4f} | Template ID: {row['template_id']}\")\n",
    "    print(f\"  Template: {row['template'][:100]}...\")\n",
    "    print(f\"  Params: {row['parameters'][:80]}\")\n",
    "    print(f\"  Raw: {row['raw_log'][:80]}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Analyze templates in anomalies\n",
    "print(f\"\\nðŸ“‹ Template Distribution in Anomalies:\")\n",
    "template_in_anomalies = anomalies['template_id'].value_counts()\n",
    "print(f\"Unique templates in anomalies: {len(template_in_anomalies)}\")\n",
    "if len(template_in_anomalies) > 0:\n",
    "    print(f\"Most common anomalous template ID: {template_in_anomalies.index[0]} ({template_in_anomalies.iloc[0]} occurrences)\")\n",
    "\n",
    "# Save results\n",
    "output_csv = f\"{CHECKPOINT_DIR}/test_anomaly_results.csv\"\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to: {output_csv}\")\n",
    "\n",
    "# Save anomalies separately\n",
    "anomalies_csv = f\"{CHECKPOINT_DIR}/anomalies_only.csv\"\n",
    "anomalies.to_csv(anomalies_csv, index=False)\n",
    "print(f\"ðŸ’¾ Anomalies saved to: {anomalies_csv}\")\n",
    "\n",
    "# Save templates\n",
    "templates_output = f\"{CHECKPOINT_DIR}/drain_templates.json\"\n",
    "templates_dict = {\n",
    "    str(tid): \" \".join(tokens) \n",
    "    for tid, tokens in drain_parser.templates.items()\n",
    "}\n",
    "with open(templates_output, 'w') as f:\n",
    "    json.dump(templates_dict, f, indent=2)\n",
    "print(f\"ðŸ’¾ Templates saved to: {templates_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logs-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
