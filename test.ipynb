{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50e997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================\n",
    "\n",
    "# --- PATHS ---\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "MODEL_FILE = \"model.pth\" # Or \"autoencoder_epoch_X.pth\"\n",
    "VOCAB_FILE = os.path.join(CHECKPOINT_DIR, \"vocab_full.json\")\n",
    "\n",
    "# --- TEST FILE (CHANGE THIS!) ---\n",
    "# Point this to the text file you want to test (ex: a new log file)\n",
    "TEST_FILE_PATH = r\"data/kafka/log-2018-06-01/129546168_2018-05-31-23-37-13.txt\"\n",
    "# If you want to test the training set, put a path here instead:\n",
    "# TEST_FILE_PATH = \"data/kafka/log-2018-06-01/129546169_2018-05-31-23-46-03.txt\"\n",
    "\n",
    "# --- CONFIG ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Regex Patterns (Same as training)\n",
    "PATTERNS_PY = {\n",
    "    r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}Z\": \"[TIMESTAMP_ISO]\",\n",
    "    r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\": \"[TIMESTAMP]\",\n",
    "    r\"PID\\s+\\d+\": \"PID [PID_NUM]\",\n",
    "    r\"\\[\\d+\\]\": \"[ID]\",\n",
    "    r\"(?:\\d{1,3}\\.){3}\\d{1,3}\": \"[IP_ADDR]\",\n",
    "    r\"\\s+\": \" \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fd4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 2. MODEL DEFINITION (Must be identical to training)\n",
    "# ==========================\n",
    "\n",
    "class LogAutoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=48, latent_dim=24, max_len=128):\n",
    "        super(LogAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embed_dim, latent_dim, batch_first=True, num_layers=1)\n",
    "        self.decoder = nn.LSTM(latent_dim, embed_dim, batch_first=True, num_layers=1)\n",
    "        self.output_linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.encoder(embedded)\n",
    "        latent = h_n.squeeze(0)\n",
    "        latent_repeated = latent.unsqueeze(1).expand(-1, self.max_len, -1)\n",
    "        # Decoder expects hidden/state of size (num_layers, batch, embed_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.embed_dim, device=x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.embed_dim, device=x.device)\n",
    "        decoded, _ = self.decoder(latent_repeated, (h0, c0))\n",
    "        reconstruction = self.output_linear(decoded)\n",
    "        return reconstruction, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0099d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3. HELPER FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Apply regex patterns to clean a single log.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    for pat, repl in PATTERNS_PY.items():\n",
    "        text = re.sub(pat, repl, text)\n",
    "    return text\n",
    "\n",
    "def text_to_ids(text, vocab, unk_id, pad_id, max_len):\n",
    "    \"\"\"Convert text string to padded IDs.\"\"\"\n",
    "    if not isinstance(text, str): return [pad_id] * max_len\n",
    "    tokens = text.split()\n",
    "    ids = [vocab.get(word, unk_id) for word in tokens]\n",
    "    \n",
    "    # Truncate or Pad\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    else:\n",
    "        ids = ids + [pad_id] * (max_len - len(ids))\n",
    "    \n",
    "    return ids\n",
    "\n",
    "def calculate_reconstruction_loss(model, input_ids, vocab_size, criterion, device):\n",
    "    \"\"\"Pass data through model and calculate loss (Anomaly Score).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = model(input_tensor)\n",
    "        \n",
    "        # Calculate loss\n",
    "        input_flat = input_tensor.view(-1)\n",
    "        reconstruction_flat = reconstruction.view(-1, vocab_size)\n",
    "        \n",
    "        loss = criterion(reconstruction_flat, input_flat)\n",
    "        \n",
    "    return loss.item(), latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7332745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Loading Model: model.pth\n",
      "ðŸ“š Loading Vocab: checkpoints\\vocab_full.json\n",
      "âœ… Vocab loaded. Size: 50000\n",
      "âœ… Model weights loaded (direct).\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 4. LOADING MODEL & VOCAB\n",
    "# ==========================\n",
    "\n",
    "print(f\"ðŸ¤– Loading Model: {MODEL_FILE}\")\n",
    "print(f\"ðŸ“š Loading Vocab: {VOCAB_FILE}\")\n",
    "\n",
    "# Load Vocab\n",
    "with open(VOCAB_FILE, 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "vocab = vocab_data['vocab']\n",
    "vocab_size = vocab_data['size']\n",
    "unk_id = vocab[vocab_data['unk_token']]\n",
    "pad_id = vocab[vocab_data['pad_token']]\n",
    "\n",
    "print(f\"âœ… Vocab loaded. Size: {vocab_size}\")\n",
    "\n",
    "# Load Model\n",
    "model = LogAutoEncoder(vocab_size=vocab_size).to(DEVICE)\n",
    "\n",
    "# Load Checkpoint\n",
    "try:\n",
    "    checkpoint = torch.load(MODEL_FILE, map_location=DEVICE)\n",
    "    \n",
    "    # Check if checkpoint has 'model_state_dict' or is just weights\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"âœ… Model weights loaded from checkpoint.\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(f\"âœ… Model weights loaded (direct).\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Model file not found: {MODEL_FILE}\")\n",
    "    exit()\n",
    "\n",
    "# Setup Loss function (Must match training: ignore padding)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a352b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-log test result:\n",
      "Raw (first 120 chars): 16:37:15     INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test\n",
      "16:37:15     INFO - Run as scripts...\n",
      "Cleaned (first 120 chars): 16:37:15 INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test 16:37:15 INFO - Run as scripts/scripts...\n",
      "Anomaly score: 11.4067\n"
     ]
    }
   ],
   "source": [
    "# Quick single-log test (no file read)\n",
    "SAMPLE_LOG = \"\"\"16:37:15     INFO - MultiFileLogger online at 20180531 16:37:15 in /builds/slave/test\\n16:37:15     INFO - Run as scripts/scripts/desktop_unittest.py --cfg unittests/mac_unittest.py --mochitest-suite browser-chrome-chunked --e10s --total-chunks 7 --this-chunk 5 --blob-upload-branch mozilla-esr52 --download-symbols true\\n16:37:15     INFO - Dumping config to /builds/slave/test/logs/localconfig.json.\\n16:37:15     INFO - {'all_cppunittest_suites': {'cppunittest': ('tests/cppunittest',)},\\n16:37:15     INFO -  'all_gtest_suites': {'gtest': ()},\\n16:37:15     INFO -  'all_jittest_suites': {'jittest': ()},\\n16:37:15     INFO -  'all_mochitest_suites': {'a11y': ('--flavor=a11y',),\\n16:37:15     INFO -                           'browser-chrome': ('--flavor=browser',),\\n16:37:15     INFO -                           'browser-chrome-addons': ('--flavor=browser',\\n16:37:15     INFO -                                                     '--chunk-by-runtime',\\n16:37:15     INFO -                                                     '--tag=addons'),\\n16:37:15     INFO -                           'browser-chrome-chunked': ('--flavor=browser',\\n16:37:15     INFO -                                                      '--chunk-by-runtime'),\\n16:37:15     INFO -                           'browser-chrome-clipboard': ('--flavor=browser',\\n16:37:15     INFO -                                                        '--subsuite=clipboard'),\\n16:37:15     INFO -                           'browser-chrome-gpu': ('--flavor=browser',\\n16:37:15     INFO -                                                  '--subsuite=gpu'),\\n16:37:15     INFO -                           'browser-chrome-screenshots': ('--flavor=browser',\\n16:37:15     INFO -                                                          '--subsuite=screenshots'),\\n16:37:15     INFO -                           'chrome': ('--flavor=chrome',),\\n16:37:15     INFO -                           'chrome-chunked': ('--flavor=chrome',\\n16:37:15     INFO -                                              '--chunk-by-dir=4'),\\n16:37:15     INFO -                           'chrome-clipboard': ('--flavor=chrome',\\n16:37:15     INFO -                                                '--subsuite=clipboard'),\\n16:37:15     INFO -                           'chrome-gpu': ('--flavor=chrome', '--subsuite=gpu'),\\n16:37:15     INFO -                           'jetpack-addon': ('--flavor=jetpack-addon',),\\n16:37:15     INFO -                           'jetpack-package': ('--flavor=jetpack-package',),\\n16:37:15     INFO -                           'jetpack-package-clipboard': ('--flavor=jetpack-package',\\n16:37:15     INFO -                                                         '--subsuite=clipboard'),\\n16:37:15     INFO -                           'mochitest-devtools-chrome': ('--flavor=browser',\\n16:37:15     INFO -                                                         '--subsuite=devtools'),\\n16:37:15     INFO -                           'mochitest-devtools-chrome-chunked': ('--flavor=browser',\\n16:37:15     INFO -                                                                 '--subsuite=devtools',\\n16:37:15     INFO -                                                                 '--chunk-by-runtime'),\\n16:37:15     INFO -                           'mochitest-gl': ('--subsuite=webgl',),\\n16:37:15     INFO -                           'mochitest-media': ('--subsuite=media',),\\n16:37:15     INFO -                           'plain': (),\\n16:37:15     INFO -                           'plain-chunked': ('--chunk-by-dir=4',),\\n16:37:15     INFO -                           'plain-clipboard': ('--subsuite=clipboard',),\\n16:37:15     INFO -                           'plain-gpu': ('--subsuite=gpu',)},\\n16:37:15     INFO -  'all_mozbase_suites': {'mozbase': ()},\\n16:37:15     INFO -  'all_reftest_suites': {'crashtest': {'options': ('--suite=crashtest',),\\n16:37:15     INFO -                                       'tests': ('tests/reftest/tests/testing/crashtest/crashtests.list',)},\\n16:37:15     INFO -                         'jsreftest': {'options': ('--extra-profile-file=tests/jsreftest/tests/user.js',),\\n16:37:15     INFO -                                       'tests': ('tests/jsreftest/tests/jstests.list',)},\\n16:37:15     INFO -                         'reftest': {'options': ('--suite=reftest',),\\n16:37:15     INFO -                                     'tests': ('tests/reftest/tests/layout/reftests/reftest.list',)}},\\n16:37:15     INFO -  'all_xpcshell_suites': {'xpcshell': {'options': ('--xpcshell=%(abs_app_dir)s/xpcshell',\\n16:37:15     INFO -                                                   '--manifest=tests/xpcshell/tests/xpcshell.ini'),\\n16:37:15     INFO -                                       'tests': ()},\\n16:37:15     INFO -                          'xpcshell-addons': {'options': ('--xpcshell=%(abs_app_dir)s/xpcshell',\\n16:37:15     INFO -                                                          '--tag=addons',\\n16:37:15     INFO -                                                          '--manifest=tests/xpcshell/tests/xpcshell.ini'),\\n16:37:15     INFO -                                              'tests': ()}},\\n16:37:15     INFO -  'allow_software_gl_layers': False,\\n16:37:15     INFO -  'append_to_log': False,\\n16:37:15     INFO -  'base_work_dir': '/builds/slave/test',\\n16:37:15     INFO -  'blob_upload_branch': 'mozilla-esr52',\\n16:37:15     INFO -  'blob_uploader_auth_file': '/builds/slave/test/oauth.txt',\\n16:37:15     INFO -  'buildbot_json_path': 'buildprops.json',\\n16:37:15     INFO -  'buildbot_max_log_size': 52428800,\\n16:37:15     INFO -  'code_coverage': False\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cleaned = clean_text(SAMPLE_LOG)\n",
    "    ids = text_to_ids(cleaned, vocab, unk_id, pad_id, MAX_LEN)\n",
    "    \n",
    "    # Fix: Use the calculate_reconstruction_loss function which handles the model correctly\n",
    "    loss, latent = calculate_reconstruction_loss(model, ids, vocab_size, criterion, DEVICE)\n",
    "\n",
    "print(\"Single-log test result:\")\n",
    "print(f\"Raw (first 120 chars): {SAMPLE_LOG[:120]}...\")\n",
    "print(f\"Cleaned (first 120 chars): {cleaned[:120]}...\")\n",
    "print(f\"Anomaly score: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6c61fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Reading file: data/kafka/log-2018-06-01/129546168_2018-05-31-23-37-13.txt\n",
      "ðŸ”„ Processing & Calculating Anomaly Scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd0f64e80634166978d949c1020ab24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Logs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================\n",
    "# 5. RUNNING THE TEST\n",
    "# ==========================\n",
    "\n",
    "# Ensure file exists\n",
    "if not os.path.exists(TEST_FILE_PATH):\n",
    "    print(f\"âŒ Test file not found: {TEST_FILE_PATH}\")\n",
    "    # Fallback: Try listing directory\n",
    "    print(f\"   Looking in: {os.path.dirname(TEST_FILE_PATH)}\")\n",
    "    if os.path.exists(os.path.dirname(TEST_FILE_PATH)):\n",
    "        files = os.listdir(os.path.dirname(TEST_FILE_PATH))\n",
    "        print(f\"   Files found: {files[:5]}\")\n",
    "    exit()\n",
    "\n",
    "# Process file\n",
    "results = []\n",
    "\n",
    "print(f\"ðŸ“‚ Reading file: {TEST_FILE_PATH}\")\n",
    "print(f\"ðŸ”„ Processing & Calculating Anomaly Scores...\")\n",
    "\n",
    "# We read line by line to avoid OOM\n",
    "try:\n",
    "    with open(TEST_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in tqdm(f, desc=\"Testing Logs\"):\n",
    "            if not line.strip(): continue\n",
    "            \n",
    "            # 1. Clean\n",
    "            cleaned = clean_text(line)\n",
    "            \n",
    "            # 2. Encode\n",
    "            ids = text_to_ids(cleaned, vocab, unk_id, pad_id, MAX_LEN)\n",
    "            \n",
    "            # 3. Predict\n",
    "            loss, latent = calculate_reconstruction_loss(model, ids, vocab_size, criterion, DEVICE)\n",
    "            \n",
    "            results.append({\n",
    "                'raw_log': line,\n",
    "                'cleaned_log': cleaned,\n",
    "                'anomaly_score': loss,\n",
    "                'latent_vector': latent.cpu().numpy().tolist() # Convert to list for saving\n",
    "            })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error processing file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a6885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308e5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š ANOMALY SCORES SUMMARY\n",
      "==================================================\n",
      "Mean Score:    3.2490\n",
      "Std Deviation:   1.4185\n",
      "Max Score:      14.0555\n",
      "Threshold (M+3*SD): 7.5045\n",
      "==================================================\n",
      "\n",
      "ðŸš¨ Found 318 Anomalies (Score > 7.5045)\n",
      "==================================================\n",
      "Top 5 Most Anomalous Logs:\n",
      "--------------------------------------------------\n",
      "Score: 14.0555 | Log: Saving to: 'archiver_client.py'\n",
      "\n",
      "Score: 11.8500 | Log: Connecting to hg.mozilla.org|63.245.215.102|:443... connected.\n",
      "\n",
      "Score: 11.6276 | Log: rm -f oauth.txt\n",
      "\n",
      "Score: 11.6031 | Log: rm -rf scripts properties\n",
      "\n",
      "Score: 11.3854 | Log:   PROPERTIES_FILE=/builds/slave/test/buildprops.json\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ’¾ Results saved to: checkpoints/test_anomaly_results.csv\n",
      "ðŸ’¾ Anomalies saved to: checkpoints/anomalies_only.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 6. ANALYZE & VISUALIZATION\n",
    "# ==========================\n",
    "\n",
    "# Convert to Pandas for analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Calculate Statistics\n",
    "mean_score = df_results['anomaly_score'].mean()\n",
    "std_score = df_results['anomaly_score'].std()\n",
    "max_score = df_results['anomaly_score'].max()\n",
    "\n",
    "# Define Threshold (Mean + 3*Std is a common heuristic)\n",
    "anomaly_threshold = mean_score + 3 * std_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"ðŸ“Š ANOMALY SCORES SUMMARY\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Mean Score:    {mean_score:.4f}\")\n",
    "print(f\"Std Deviation:   {std_score:.4f}\")\n",
    "print(f\"Max Score:      {max_score:.4f}\")\n",
    "print(f\"Threshold (M+3*SD): {anomaly_threshold:.4f}\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Highlight Anomalies (Logs above threshold)\n",
    "anomalies = df_results[df_results['anomaly_score'] > anomaly_threshold]\n",
    "\n",
    "print(f\"\\nðŸš¨ Found {len(anomalies)} Anomalies (Score > {anomaly_threshold:.4f})\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "# Show top 5 most anomalous logs\n",
    "print(\"Top 5 Most Anomalous Logs:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in anomalies.nlargest(5, 'anomaly_score').iterrows():\n",
    "    print(f\"Score: {row['anomaly_score']:.4f} | Log: {row['raw_log'][:80]}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save results\n",
    "output_csv = f\"{CHECKPOINT_DIR}/test_anomaly_results.csv\"\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to: {output_csv}\")\n",
    "\n",
    "# Save anomalies separately\n",
    "anomalies_csv = f\"{CHECKPOINT_DIR}/anomalies_only.csv\"\n",
    "anomalies.to_csv(anomalies_csv, index=False)\n",
    "print(f\"ðŸ’¾ Anomalies saved to: {anomalies_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logs-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
